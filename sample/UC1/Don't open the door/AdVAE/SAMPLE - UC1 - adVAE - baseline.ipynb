{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "if '..' not in path:\n",
    "    path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Dataset...\n",
      "Number of data\n",
      "Training: 1000, Test: 2000\n",
      "\n",
      "Information of data\n",
      "Shape  Height: 28, Width: 28, Channel: 1\n",
      "Value  Min: 0.000, Max: 255.000\n",
      "Class  10\n",
      "Normalization: True\n",
      "(from 0.000-255.000 to 0.000-1.000)\n",
      "\n",
      "Initializing Neural Network...\n",
      "WARNING:tensorflow:From ../adVAE/neuralnet.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../adVAE/neuralnet.py:134: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "\n",
      "Encoder-1\n",
      "WARNING:tensorflow:From ../adVAE/neuralnet.py:273: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Conv (?, 28, 28, 1) -> (?, 28, 28, 16)\n",
      "Conv (?, 28, 28, 16) -> (?, 28, 28, 16)\n",
      "Max-Pool (?, 28, 28, 16) -> (?, 14, 14, 16)\n",
      "Encoder-2\n",
      "Conv (?, 14, 14, 16) -> (?, 14, 14, 32)\n",
      "Conv (?, 14, 14, 32) -> (?, 14, 14, 32)\n",
      "Max-Pool (?, 14, 14, 32) -> (?, 7, 7, 32)\n",
      "Encoder-3\n",
      "Conv (?, 7, 7, 32) -> (?, 7, 7, 64)\n",
      "Conv (?, 7, 7, 64) -> (?, 7, 7, 64)\n",
      "Encoder-Dense\n",
      "Full-Con (?, 3136) -> (?, 512)\n",
      "Full-Con (?, 512) -> (?, 256)\n",
      "\n",
      "Transformer-Dense\n",
      "Full-Con (?, 128) -> (?, 256)\n",
      "\n",
      "Generator-Dense\n",
      "Full-Con (?, 128) -> (?, 512)\n",
      "Full-Con (?, 512) -> (?, 3136)\n",
      "Generator-1\n",
      "Conv (?, 7, 7, 64) -> (?, 7, 7, 64)\n",
      "Conv (?, 7, 7, 64) -> (?, 7, 7, 64)\n",
      "Generator-2\n",
      "Conv-Tr (?, 7, 7, 64) -> (?, 14, 14, 32)\n",
      "Conv (?, 14, 14, 32) -> (?, 14, 14, 32)\n",
      "Generator-3\n",
      "Conv-Tr (?, 14, 14, 32) -> (?, 28, 28, 16)\n",
      "Conv (?, 28, 28, 16) -> (?, 28, 28, 16)\n",
      "Conv (?, 28, 28, 16) -> (?, 28, 28, 1)\n",
      "\n",
      "Generator-Dense\n",
      "Full-Con (?, 128) -> (?, 512)\n",
      "Full-Con (?, 512) -> (?, 3136)\n",
      "Generator-1\n",
      "Conv (?, 7, 7, 64) -> (?, 7, 7, 64)\n",
      "Conv (?, 7, 7, 64) -> (?, 7, 7, 64)\n",
      "Generator-2\n",
      "Conv-Tr (?, 7, 7, 64) -> (?, 14, 14, 32)\n",
      "Conv (?, 14, 14, 32) -> (?, 14, 14, 32)\n",
      "Generator-3\n",
      "Conv-Tr (?, 14, 14, 32) -> (?, 28, 28, 16)\n",
      "Conv (?, 28, 28, 16) -> (?, 28, 28, 16)\n",
      "Conv (?, 28, 28, 16) -> (?, 28, 28, 1)\n",
      "\n",
      "Encoder-1\n",
      "Conv (?, 28, 28, 1) -> (?, 28, 28, 16)\n",
      "Conv (?, 28, 28, 16) -> (?, 28, 28, 16)\n",
      "Max-Pool (?, 28, 28, 16) -> (?, 14, 14, 16)\n",
      "Encoder-2\n",
      "Conv (?, 14, 14, 16) -> (?, 14, 14, 32)\n",
      "Conv (?, 14, 14, 32) -> (?, 14, 14, 32)\n",
      "Max-Pool (?, 14, 14, 32) -> (?, 7, 7, 32)\n",
      "Encoder-3\n",
      "Conv (?, 7, 7, 32) -> (?, 7, 7, 64)\n",
      "Conv (?, 7, 7, 64) -> (?, 7, 7, 64)\n",
      "Encoder-Dense\n",
      "Full-Con (?, 3136) -> (?, 512)\n",
      "Full-Con (?, 512) -> (?, 256)\n",
      "\n",
      "Encoder-1\n",
      "Conv (?, 28, 28, 1) -> (?, 28, 28, 16)\n",
      "Conv (?, 28, 28, 16) -> (?, 28, 28, 16)\n",
      "Max-Pool (?, 28, 28, 16) -> (?, 14, 14, 16)\n",
      "Encoder-2\n",
      "Conv (?, 14, 14, 16) -> (?, 14, 14, 32)\n",
      "Conv (?, 14, 14, 32) -> (?, 14, 14, 32)\n",
      "Max-Pool (?, 14, 14, 32) -> (?, 7, 7, 32)\n",
      "Encoder-3\n",
      "Conv (?, 7, 7, 32) -> (?, 7, 7, 64)\n",
      "Conv (?, 7, 7, 64) -> (?, 7, 7, 64)\n",
      "Encoder-Dense\n",
      "Full-Con (?, 3136) -> (?, 512)\n",
      "Full-Con (?, 512) -> (?, 256)\n",
      "\n",
      "Variables (T and G)\n",
      "<tf.Variable 'transformer/tra_fullcon1_w:0' shape=(128, 256) dtype=float32_ref>\n",
      "<tf.Variable 'transformer/tra_fullcon1_b:0' shape=(256,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_fullcon2_w:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_fullcon2_b:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_fullcon3_w:0' shape=(512, 3136) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_fullcon3_b:0' shape=(3136,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt1_1_w:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt1_1_b:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt1_2_w:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt1_2_b:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt2_1_w:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt2_1_b:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt2_2_w:0' shape=(3, 3, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt2_2_b:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt3_1_w:0' shape=(3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt3_1_b:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt3_2_w:0' shape=(3, 3, 16, 16) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt3_2_b:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt3_3_w:0' shape=(3, 3, 16, 1) dtype=float32_ref>\n",
      "<tf.Variable 'generator/gen_convt3_3_b:0' shape=(1,) dtype=float32_ref>\n",
      "\n",
      "Variables (E)\n",
      "<tf.Variable 'encoder/enc_conv1_1_w:0' shape=(3, 3, 1, 16) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1_1_b:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1_2_w:0' shape=(3, 3, 16, 16) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1_2_b:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv2_1_w:0' shape=(3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv2_1_b:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv2_2_w:0' shape=(3, 3, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv2_2_b:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv3_1_w:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv3_1_b:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv3_2_w:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv3_2_b:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_fullcon1_w:0' shape=(3136, 512) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_fullcon1_b:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_z_param_w:0' shape=(512, 256) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_z_param_b:0' shape=(256,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From /home/vieri/anaconda3/envs/SAMPLE_tf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n",
      "Training to 100 epochs (32 of minibatch size)\n",
      "Epoch [0 / 100] (32 iteration) Loss  T:4360.489, G:4236.576, E:424.499, Tot:9021.565\n",
      "Epoch [1 / 100] (64 iteration) Loss  T:4414.978, G:3711.560, E:369.734, Tot:8496.272\n",
      "Epoch [2 / 100] (96 iteration) Loss  T:4137.023, G:3702.343, E:369.056, Tot:8208.423\n",
      "Epoch [3 / 100] (128 iteration) Loss  T:4110.568, G:3710.882, E:369.918, Tot:8191.368\n",
      "Epoch [4 / 100] (160 iteration) Loss  T:4091.283, G:3693.011, E:368.148, Tot:8152.443\n",
      "Epoch [5 / 100] (192 iteration) Loss  T:4049.454, G:3681.268, E:367.024, Tot:8097.745\n",
      "Epoch [6 / 100] (224 iteration) Loss  T:4001.326, G:3689.878, E:367.953, Tot:8059.157\n",
      "Epoch [7 / 100] (256 iteration) Loss  T:3878.223, G:3592.662, E:358.893, Tot:7829.778\n",
      "Epoch [8 / 100] (288 iteration) Loss  T:3852.851, G:3505.372, E:350.621, Tot:7708.844\n",
      "Epoch [9 / 100] (320 iteration) Loss  T:3821.757, G:3429.927, E:343.138, Tot:7594.822\n",
      "Epoch [10 / 100] (352 iteration) Loss  T:3974.242, G:3440.573, E:344.314, Tot:7759.129\n",
      "Epoch [11 / 100] (384 iteration) Loss  T:4032.626, G:2999.116, E:300.871, Tot:7332.613\n",
      "Epoch [12 / 100] (416 iteration) Loss  T:3999.475, G:2865.538, E:288.100, Tot:7153.112\n",
      "Epoch [13 / 100] (448 iteration) Loss  T:4133.676, G:2866.424, E:288.070, Tot:7288.170\n",
      "Epoch [14 / 100] (480 iteration) Loss  T:4106.881, G:2855.241, E:286.591, Tot:7248.712\n",
      "Epoch [15 / 100] (512 iteration) Loss  T:3924.883, G:2845.084, E:285.742, Tot:7055.709\n",
      "Epoch [16 / 100] (544 iteration) Loss  T:3868.677, G:2820.291, E:283.223, Tot:6972.192\n",
      "Epoch [17 / 100] (576 iteration) Loss  T:4080.229, G:2823.242, E:283.227, Tot:7186.698\n",
      "Epoch [18 / 100] (608 iteration) Loss  T:4017.875, G:2821.433, E:283.177, Tot:7122.484\n",
      "Epoch [19 / 100] (640 iteration) Loss  T:3971.047, G:2790.347, E:279.763, Tot:7041.158\n",
      "Epoch [20 / 100] (672 iteration) Loss  T:4034.851, G:2824.701, E:283.245, Tot:7142.796\n",
      "Epoch [21 / 100] (704 iteration) Loss  T:3894.685, G:2822.944, E:283.306, Tot:7000.935\n",
      "Epoch [22 / 100] (736 iteration) Loss  T:3737.390, G:2778.781, E:278.702, Tot:6794.873\n",
      "Epoch [23 / 100] (768 iteration) Loss  T:3839.580, G:2788.264, E:279.562, Tot:6907.405\n",
      "Epoch [24 / 100] (800 iteration) Loss  T:3849.145, G:2543.612, E:254.986, Tot:6647.743\n",
      "Epoch [25 / 100] (832 iteration) Loss  T:3806.735, G:2517.655, E:252.517, Tot:6576.907\n",
      "Epoch [26 / 100] (864 iteration) Loss  T:3871.027, G:2531.690, E:253.992, Tot:6656.709\n",
      "Epoch [27 / 100] (896 iteration) Loss  T:3668.922, G:2515.690, E:252.259, Tot:6436.872\n",
      "Epoch [28 / 100] (928 iteration) Loss  T:3676.716, G:2513.504, E:251.866, Tot:6442.085\n",
      "Epoch [29 / 100] (960 iteration) Loss  T:3791.303, G:2504.888, E:251.019, Tot:6547.209\n",
      "Epoch [30 / 100] (992 iteration) Loss  T:3767.704, G:2516.292, E:252.466, Tot:6536.461\n",
      "Epoch [31 / 100] (1024 iteration) Loss  T:3713.744, G:2494.234, E:249.933, Tot:6457.911\n",
      "Epoch [32 / 100] (1056 iteration) Loss  T:3550.259, G:2540.928, E:254.736, Tot:6345.923\n",
      "Epoch [33 / 100] (1088 iteration) Loss  T:3596.237, G:2495.306, E:250.090, Tot:6341.633\n",
      "Epoch [34 / 100] (1120 iteration) Loss  T:3724.442, G:2506.652, E:251.321, Tot:6482.416\n",
      "Epoch [35 / 100] (1152 iteration) Loss  T:3620.755, G:2500.288, E:250.494, Tot:6371.538\n",
      "Epoch [36 / 100] (1184 iteration) Loss  T:3649.336, G:2491.551, E:249.667, Tot:6390.554\n",
      "Epoch [37 / 100] (1216 iteration) Loss  T:3622.760, G:2504.474, E:251.015, Tot:6378.248\n",
      "Epoch [38 / 100] (1248 iteration) Loss  T:3689.273, G:2506.698, E:251.233, Tot:6447.204\n",
      "Epoch [39 / 100] (1280 iteration) Loss  T:3607.057, G:2492.441, E:249.784, Tot:6349.283\n",
      "Epoch [40 / 100] (1312 iteration) Loss  T:3623.835, G:2485.526, E:248.886, Tot:6358.248\n",
      "Epoch [41 / 100] (1344 iteration) Loss  T:3524.063, G:2479.523, E:248.456, Tot:6252.042\n",
      "Epoch [42 / 100] (1376 iteration) Loss  T:3470.894, G:2488.828, E:249.414, Tot:6209.136\n",
      "Epoch [43 / 100] (1408 iteration) Loss  T:3452.030, G:2493.404, E:249.943, Tot:6195.377\n",
      "Epoch [44 / 100] (1440 iteration) Loss  T:3441.820, G:2477.842, E:248.269, Tot:6167.931\n",
      "Epoch [45 / 100] (1472 iteration) Loss  T:3493.998, G:2483.054, E:248.815, Tot:6225.867\n",
      "Epoch [46 / 100] (1504 iteration) Loss  T:3460.669, G:2497.388, E:250.308, Tot:6208.364\n",
      "Epoch [47 / 100] (1536 iteration) Loss  T:3388.414, G:2474.957, E:247.937, Tot:6111.308\n",
      "Epoch [48 / 100] (1568 iteration) Loss  T:3384.261, G:2507.319, E:251.048, Tot:6142.629\n",
      "Epoch [49 / 100] (1600 iteration) Loss  T:3291.689, G:2475.314, E:247.947, Tot:6014.950\n",
      "Epoch [50 / 100] (1632 iteration) Loss  T:3379.052, G:2472.230, E:247.620, Tot:6098.901\n",
      "Epoch [51 / 100] (1664 iteration) Loss  T:3382.477, G:2464.756, E:246.825, Tot:6094.058\n",
      "Epoch [52 / 100] (1696 iteration) Loss  T:3306.935, G:2469.397, E:247.206, Tot:6023.538\n",
      "Epoch [53 / 100] (1728 iteration) Loss  T:3231.575, G:2460.241, E:246.300, Tot:5938.116\n",
      "Epoch [54 / 100] (1760 iteration) Loss  T:3252.101, G:2466.103, E:246.941, Tot:5965.145\n",
      "Epoch [55 / 100] (1792 iteration) Loss  T:3370.568, G:2478.109, E:248.022, Tot:6096.699\n",
      "Epoch [56 / 100] (1824 iteration) Loss  T:3282.495, G:2465.556, E:246.696, Tot:5994.747\n",
      "Epoch [57 / 100] (1856 iteration) Loss  T:3220.631, G:2457.701, E:245.979, Tot:5924.312\n",
      "Epoch [58 / 100] (1888 iteration) Loss  T:3221.170, G:2468.164, E:247.062, Tot:5936.396\n",
      "Epoch [59 / 100] (1920 iteration) Loss  T:3250.815, G:2468.434, E:247.024, Tot:5966.272\n",
      "Epoch [60 / 100] (1952 iteration) Loss  T:3232.174, G:2457.012, E:245.950, Tot:5935.136\n",
      "Epoch [61 / 100] (1984 iteration) Loss  T:3272.271, G:2492.461, E:249.529, Tot:6014.262\n",
      "Epoch [62 / 100] (2016 iteration) Loss  T:3246.164, G:2464.041, E:246.704, Tot:5956.909\n",
      "Epoch [63 / 100] (2048 iteration) Loss  T:3167.467, G:2449.137, E:245.173, Tot:5861.777\n",
      "Epoch [64 / 100] (2080 iteration) Loss  T:3087.685, G:2457.262, E:246.004, Tot:5790.951\n",
      "Epoch [65 / 100] (2112 iteration) Loss  T:3109.563, G:2465.408, E:246.878, Tot:5821.849\n",
      "Epoch [66 / 100] (2144 iteration) Loss  T:3145.712, G:2476.922, E:248.029, Tot:5870.664\n",
      "Epoch [67 / 100] (2176 iteration) Loss  T:3114.095, G:2458.071, E:246.100, Tot:5818.266\n",
      "Epoch [68 / 100] (2208 iteration) Loss  T:3162.835, G:2461.071, E:246.379, Tot:5870.285\n",
      "Epoch [69 / 100] (2240 iteration) Loss  T:3095.946, G:2466.430, E:246.851, Tot:5809.228\n",
      "Epoch [70 / 100] (2272 iteration) Loss  T:3198.767, G:2467.373, E:247.181, Tot:5913.320\n",
      "Epoch [71 / 100] (2304 iteration) Loss  T:3128.779, G:2471.969, E:247.511, Tot:5848.259\n",
      "Epoch [72 / 100] (2336 iteration) Loss  T:3143.262, G:2446.485, E:244.867, Tot:5834.614\n",
      "Epoch [73 / 100] (2368 iteration) Loss  T:3106.474, G:2461.579, E:246.598, Tot:5814.651\n",
      "Epoch [74 / 100] (2400 iteration) Loss  T:3110.323, G:2453.867, E:245.619, Tot:5809.809\n",
      "Epoch [75 / 100] (2432 iteration) Loss  T:3056.397, G:2456.745, E:245.945, Tot:5759.086\n",
      "Epoch [76 / 100] (2464 iteration) Loss  T:2974.647, G:2446.118, E:244.763, Tot:5665.528\n",
      "Epoch [77 / 100] (2496 iteration) Loss  T:3093.098, G:2482.247, E:248.458, Tot:5823.804\n",
      "Epoch [78 / 100] (2528 iteration) Loss  T:3005.149, G:2458.067, E:246.036, Tot:5709.251\n",
      "Epoch [79 / 100] (2560 iteration) Loss  T:3006.599, G:2471.155, E:247.334, Tot:5725.088\n",
      "Epoch [80 / 100] (2592 iteration) Loss  T:3068.697, G:2439.222, E:244.104, Tot:5752.022\n",
      "Epoch [81 / 100] (2624 iteration) Loss  T:3017.793, G:2446.123, E:244.835, Tot:5708.751\n",
      "Epoch [82 / 100] (2656 iteration) Loss  T:2940.249, G:2444.261, E:244.643, Tot:5629.152\n",
      "Epoch [83 / 100] (2688 iteration) Loss  T:3031.634, G:2450.298, E:245.288, Tot:5727.220\n",
      "Epoch [84 / 100] (2720 iteration) Loss  T:2856.899, G:2457.950, E:246.051, Tot:5560.900\n",
      "Epoch [85 / 100] (2752 iteration) Loss  T:2997.553, G:2442.800, E:244.396, Tot:5684.750\n",
      "Epoch [86 / 100] (2784 iteration) Loss  T:3027.859, G:2446.790, E:244.945, Tot:5719.594\n",
      "Epoch [87 / 100] (2816 iteration) Loss  T:3038.024, G:2101.193, E:210.330, Tot:5349.547\n",
      "Epoch [88 / 100] (2848 iteration) Loss  T:3389.858, G:1487.507, E:149.506, Tot:5026.871\n",
      "Epoch [89 / 100] (2880 iteration) Loss  T:3253.864, G:1468.997, E:147.695, Tot:4870.556\n",
      "Epoch [90 / 100] (2912 iteration) Loss  T:3254.110, G:1459.032, E:146.909, Tot:4860.050\n",
      "Epoch [91 / 100] (2944 iteration) Loss  T:3173.756, G:1421.116, E:143.119, Tot:4737.992\n",
      "Epoch [92 / 100] (2976 iteration) Loss  T:3209.570, G:1418.164, E:142.268, Tot:4770.001\n",
      "Epoch [93 / 100] (3008 iteration) Loss  T:3180.156, G:1405.695, E:141.266, Tot:4727.117\n",
      "Epoch [94 / 100] (3040 iteration) Loss  T:3200.429, G:1416.961, E:142.221, Tot:4759.611\n",
      "Epoch [95 / 100] (3072 iteration) Loss  T:3201.970, G:1429.899, E:143.753, Tot:4775.622\n",
      "Epoch [96 / 100] (3104 iteration) Loss  T:3177.320, G:1400.392, E:140.717, Tot:4718.429\n",
      "Epoch [97 / 100] (3136 iteration) Loss  T:3138.895, G:1409.386, E:141.583, Tot:4689.864\n",
      "Epoch [98 / 100] (3168 iteration) Loss  T:3167.650, G:1405.996, E:141.302, Tot:4714.948\n",
      "Epoch [99 / 100] (3200 iteration) Loss  T:3096.968, G:1388.617, E:139.144, Tot:4624.729\n",
      "\n",
      "Restoring parameters\n",
      "WARNING:tensorflow:From /home/vieri/anaconda3/envs/SAMPLE_tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/vieri/notebooks/sample/adVAE/../Checkpoint/model_checker\n",
      "\n",
      "Test...\n",
      "Noraml  avg: 5.44918, std: 5.17287\n",
      "Abnoraml  avg: 64.50941, std: 25.02473\n",
      "Outlier boundary of normal data: 20.96778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, warnings, argparse\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import adVAE.datamanager as dman\n",
    "import adVAE.neuralnet as nn\n",
    "import adVAE.tf_process as tfp\n",
    "\n",
    "# parameters\n",
    "datnorm = True\n",
    "z_dim = 128\n",
    "mx = 1\n",
    "mz = 1\n",
    "lr = 1e-4\n",
    "epoch = 100\n",
    "batch = 32\n",
    "\n",
    "\n",
    "\"\"\"Initializing the dataset\"\"\"\n",
    "dataset = dman.Dataset(normalize=datnorm)\n",
    "\n",
    "\"\"\"Initializing the neural network\"\"\"\n",
    "neuralnet = nn.adVAE(height = dataset.height, width = dataset.width, channel = dataset.channel, \n",
    "                     z_dim = z_dim, mx = mx, mz = mz, leaning_rate = lr)\n",
    "\n",
    "sess_config = tf.compat.v1.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=sess_config)\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\"\"\"Process of training and test with neural network\"\"\"\n",
    "tfp.training(sess=sess, neuralnet=neuralnet, saver=saver, dataset=dataset, epochs=epoch, batch_size=batch, normalize=True)\n",
    "tfp.test(sess=sess, neuralnet=neuralnet, saver=saver, dataset=dataset, batch_size=batch)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (SAMPLE_tf)",
   "language": "python",
   "name": "sample_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
