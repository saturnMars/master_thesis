{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "if '..' not in path:\n",
    "    path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from _library.utils import SYSTEM_NAMES, SUBFOLDERS, load_datasets\n",
    "from _library.som_utils import compute_metrics\n",
    "import _library.fault_utils as fault_utils\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import _library.uc2_interpolation as interpolation_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from os import path, makedirs\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/vieri/projects/SAMPLE\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/data/vieri/projects/SAMPLE/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The photovoltaic systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Binetto 1', 'Binetto 2', 'Soleto 1', 'Soleto 2', 'Galatina'] \n",
      "SUBFOLDERS: --> ['Cleaned', '1-hour sampling', '1-hour averaged sampling', 'Residuals', 'Residuals_analytical', None]\n"
     ]
    }
   ],
   "source": [
    "print(SYSTEM_NAMES, \"\\nSUBFOLDERS: -->\", SUBFOLDERS)\n",
    "# --- 0 ---------- 1 ---------- 2 --------- 3 ---------- 4 -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Selecting the PV system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV SYSTEM --> Soleto 1\n"
     ]
    }
   ],
   "source": [
    "system_name = SYSTEM_NAMES[2]\n",
    "print(f\"PV SYSTEM --> {system_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Selecting the dataset type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = \"Residuals\" #+ \"_analytical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------- \n",
      "\t\t\t\tPV SYSTEM --> SOLETO 1 \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Loading inverter data...\n",
      "SOLETO 1: OK, component data loaded (4) --> INV1, INV2, INV3, INV4\n",
      "-------------------------------------------------------------------------------- \n",
      "FINISHED!: All datasets have been loaded. (SYS: 4 - IRR FILE: 0)\n",
      "--------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------- \n",
      "EXAMPLE --> Soleto 1: INV1 (FROM '2018-08-08' TO '2021-06-30': 1057 days).\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   Date/Time                   14277 non-null  datetime64[ns]\n",
      " 1   Iac R (A)                   14277 non-null  int64         \n",
      " 2   Iac S (A)                   14277 non-null  int64         \n",
      " 3   Iac T (A)                   14277 non-null  int64         \n",
      " 4   Vac R (V)                   14277 non-null  int64         \n",
      " 5   Vac S (V)                   14277 non-null  int64         \n",
      " 6   Vac T (V)                   14277 non-null  int64         \n",
      " 7   Pac R (kW)                  14277 non-null  int64         \n",
      " 8   E. totale (kWh)             14277 non-null  float64       \n",
      " 9   Cc 1 (A)                    14277 non-null  int64         \n",
      " 10  Vcc 1 (V)                   14277 non-null  int64         \n",
      " 11  Allarme                     14277 non-null  string        \n",
      " 12  Inverter temp. (°C)         14277 non-null  int64         \n",
      " 13  Irradiance (W/mq)           14277 non-null  int64         \n",
      " 14  Amb. Temp (°C)              14277 non-null  float64       \n",
      " 15  Humidity (%)                13852 non-null  float64       \n",
      " 16  Atmospheric Pressure (hPa)  14276 non-null  float64       \n",
      " 17  Rainfall (mm)               14277 non-null  float64       \n",
      " 18  Wind speed (m/s)            5632 non-null   float64       \n",
      " 19  Wind direction (°)          5653 non-null   float64       \n",
      " 20  Cell Temp (°C)              14277 non-null  float64       \n",
      " 21  Maxiumum Voltage (V)        14277 non-null  float64       \n",
      " 22  Maxiumum Current (A)        14277 non-null  float64       \n",
      " 23  Residuals (A)               14277 non-null  float64       \n",
      " 24  Residuals (V)               14277 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(11), string(1)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "system_path, inv_data, inv_names, *_ = load_datasets(system_name, subfolder = dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Statistic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_columns = ['Cc 1 (A)', 'Maxiumum Current (A)', 'Residuals (A)', 'Vcc 1 (V)', 'Maxiumum Voltage (V)', 'Residuals (V)',\n",
    "                    'Amb. Temp (°C)', 'Irradiance (W/mq)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------- INV1 ---------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The timetamps are now used as index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cc 1 (A)</th>\n",
       "      <th>Maxiumum Current (A)</th>\n",
       "      <th>Residuals (A)</th>\n",
       "      <th>Vcc 1 (V)</th>\n",
       "      <th>Maxiumum Voltage (V)</th>\n",
       "      <th>Residuals (V)</th>\n",
       "      <th>Amb. Temp (°C)</th>\n",
       "      <th>Irradiance (W/mq)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14277.00</td>\n",
       "      <td>14277.00</td>\n",
       "      <td>14277.00</td>\n",
       "      <td>14277.00</td>\n",
       "      <td>14277.00</td>\n",
       "      <td>14277.00</td>\n",
       "      <td>14277.00</td>\n",
       "      <td>14277.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>136.45</td>\n",
       "      <td>178.47</td>\n",
       "      <td>-42.02</td>\n",
       "      <td>321.61</td>\n",
       "      <td>463.27</td>\n",
       "      <td>-141.66</td>\n",
       "      <td>19.48</td>\n",
       "      <td>325.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>156.95</td>\n",
       "      <td>171.24</td>\n",
       "      <td>69.25</td>\n",
       "      <td>146.74</td>\n",
       "      <td>20.64</td>\n",
       "      <td>153.59</td>\n",
       "      <td>8.10</td>\n",
       "      <td>333.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-448.52</td>\n",
       "      <td>12.00</td>\n",
       "      <td>315.94</td>\n",
       "      <td>-671.88</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>25.09</td>\n",
       "      <td>-48.63</td>\n",
       "      <td>302.00</td>\n",
       "      <td>451.35</td>\n",
       "      <td>-174.34</td>\n",
       "      <td>13.25</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.00</td>\n",
       "      <td>94.03</td>\n",
       "      <td>-23.49</td>\n",
       "      <td>387.00</td>\n",
       "      <td>474.08</td>\n",
       "      <td>-67.91</td>\n",
       "      <td>18.60</td>\n",
       "      <td>173.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>259.00</td>\n",
       "      <td>347.55</td>\n",
       "      <td>-17.86</td>\n",
       "      <td>414.00</td>\n",
       "      <td>478.24</td>\n",
       "      <td>-41.19</td>\n",
       "      <td>25.87</td>\n",
       "      <td>614.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>545.00</td>\n",
       "      <td>504.07</td>\n",
       "      <td>313.30</td>\n",
       "      <td>580.00</td>\n",
       "      <td>1072.88</td>\n",
       "      <td>116.01</td>\n",
       "      <td>41.35</td>\n",
       "      <td>1336.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cc 1 (A)  Maxiumum Current (A)  Residuals (A)  Vcc 1 (V)  \\\n",
       "count  14277.00              14277.00       14277.00   14277.00   \n",
       "mean     136.45                178.47         -42.02     321.61   \n",
       "std      156.95                171.24          69.25     146.74   \n",
       "min        0.00                  0.00        -448.52      12.00   \n",
       "25%        0.00                 25.09         -48.63     302.00   \n",
       "50%       62.00                 94.03         -23.49     387.00   \n",
       "75%      259.00                347.55         -17.86     414.00   \n",
       "max      545.00                504.07         313.30     580.00   \n",
       "\n",
       "       Maxiumum Voltage (V)  Residuals (V)  Amb. Temp (°C)  Irradiance (W/mq)  \n",
       "count              14277.00       14277.00        14277.00           14277.00  \n",
       "mean                 463.27        -141.66           19.48             325.17  \n",
       "std                   20.64         153.59            8.10             333.82  \n",
       "min                  315.94        -671.88           -0.13               4.00  \n",
       "25%                  451.35        -174.34           13.25              33.00  \n",
       "50%                  474.08         -67.91           18.60             173.00  \n",
       "75%                  478.24         -41.19           25.87             614.00  \n",
       "max                 1072.88         116.01           41.35            1336.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------- INV2 ---------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The timetamps are now used as index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cc 1 (A)</th>\n",
       "      <th>Maxiumum Current (A)</th>\n",
       "      <th>Residuals (A)</th>\n",
       "      <th>Vcc 1 (V)</th>\n",
       "      <th>Maxiumum Voltage (V)</th>\n",
       "      <th>Residuals (V)</th>\n",
       "      <th>Amb. Temp (°C)</th>\n",
       "      <th>Irradiance (W/mq)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14175.00</td>\n",
       "      <td>14175.00</td>\n",
       "      <td>14175.00</td>\n",
       "      <td>14175.00</td>\n",
       "      <td>14175.00</td>\n",
       "      <td>14175.00</td>\n",
       "      <td>14175.00</td>\n",
       "      <td>14175.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>140.94</td>\n",
       "      <td>179.27</td>\n",
       "      <td>-38.33</td>\n",
       "      <td>331.96</td>\n",
       "      <td>463.16</td>\n",
       "      <td>-131.20</td>\n",
       "      <td>19.56</td>\n",
       "      <td>326.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>160.30</td>\n",
       "      <td>171.45</td>\n",
       "      <td>68.94</td>\n",
       "      <td>149.76</td>\n",
       "      <td>20.70</td>\n",
       "      <td>156.46</td>\n",
       "      <td>8.08</td>\n",
       "      <td>334.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-452.52</td>\n",
       "      <td>12.00</td>\n",
       "      <td>315.94</td>\n",
       "      <td>-664.88</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>25.16</td>\n",
       "      <td>-41.75</td>\n",
       "      <td>322.00</td>\n",
       "      <td>451.23</td>\n",
       "      <td>-154.03</td>\n",
       "      <td>13.34</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>65.00</td>\n",
       "      <td>95.21</td>\n",
       "      <td>-22.70</td>\n",
       "      <td>404.00</td>\n",
       "      <td>473.89</td>\n",
       "      <td>-52.28</td>\n",
       "      <td>18.71</td>\n",
       "      <td>175.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>268.00</td>\n",
       "      <td>348.28</td>\n",
       "      <td>-14.03</td>\n",
       "      <td>422.00</td>\n",
       "      <td>478.24</td>\n",
       "      <td>-32.98</td>\n",
       "      <td>25.94</td>\n",
       "      <td>618.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>556.00</td>\n",
       "      <td>504.07</td>\n",
       "      <td>320.30</td>\n",
       "      <td>566.00</td>\n",
       "      <td>1072.88</td>\n",
       "      <td>105.49</td>\n",
       "      <td>41.35</td>\n",
       "      <td>1336.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cc 1 (A)  Maxiumum Current (A)  Residuals (A)  Vcc 1 (V)  \\\n",
       "count  14175.00              14175.00       14175.00   14175.00   \n",
       "mean     140.94                179.27         -38.33     331.96   \n",
       "std      160.30                171.45          68.94     149.76   \n",
       "min        0.00                  0.00        -452.52      12.00   \n",
       "25%        0.00                 25.16         -41.75     322.00   \n",
       "50%       65.00                 95.21         -22.70     404.00   \n",
       "75%      268.00                348.28         -14.03     422.00   \n",
       "max      556.00                504.07         320.30     566.00   \n",
       "\n",
       "       Maxiumum Voltage (V)  Residuals (V)  Amb. Temp (°C)  Irradiance (W/mq)  \n",
       "count              14175.00       14175.00        14175.00           14175.00  \n",
       "mean                 463.16        -131.20           19.56             326.51  \n",
       "std                   20.70         156.46            8.08             334.13  \n",
       "min                  315.94        -664.88           -0.13               4.00  \n",
       "25%                  451.23        -154.03           13.34              33.00  \n",
       "50%                  473.89         -52.28           18.71             175.00  \n",
       "75%                  478.24         -32.98           25.94             618.00  \n",
       "max                 1072.88         105.49           41.35            1336.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------- INV3 ---------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The timetamps are now used as index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cc 1 (A)</th>\n",
       "      <th>Maxiumum Current (A)</th>\n",
       "      <th>Residuals (A)</th>\n",
       "      <th>Vcc 1 (V)</th>\n",
       "      <th>Maxiumum Voltage (V)</th>\n",
       "      <th>Residuals (V)</th>\n",
       "      <th>Amb. Temp (°C)</th>\n",
       "      <th>Irradiance (W/mq)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14257.00</td>\n",
       "      <td>14257.00</td>\n",
       "      <td>14257.00</td>\n",
       "      <td>14257.00</td>\n",
       "      <td>14257.00</td>\n",
       "      <td>14257.00</td>\n",
       "      <td>14257.00</td>\n",
       "      <td>14257.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>146.93</td>\n",
       "      <td>179.01</td>\n",
       "      <td>-32.08</td>\n",
       "      <td>358.16</td>\n",
       "      <td>463.22</td>\n",
       "      <td>-105.05</td>\n",
       "      <td>19.50</td>\n",
       "      <td>326.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>166.79</td>\n",
       "      <td>171.27</td>\n",
       "      <td>70.70</td>\n",
       "      <td>159.06</td>\n",
       "      <td>20.65</td>\n",
       "      <td>164.62</td>\n",
       "      <td>8.08</td>\n",
       "      <td>333.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-485.62</td>\n",
       "      <td>20.00</td>\n",
       "      <td>315.94</td>\n",
       "      <td>-614.88</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>25.15</td>\n",
       "      <td>-31.04</td>\n",
       "      <td>366.00</td>\n",
       "      <td>451.32</td>\n",
       "      <td>-110.29</td>\n",
       "      <td>13.29</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>69.00</td>\n",
       "      <td>94.98</td>\n",
       "      <td>-21.84</td>\n",
       "      <td>432.00</td>\n",
       "      <td>473.99</td>\n",
       "      <td>-21.13</td>\n",
       "      <td>18.62</td>\n",
       "      <td>175.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>280.00</td>\n",
       "      <td>347.99</td>\n",
       "      <td>-7.33</td>\n",
       "      <td>456.00</td>\n",
       "      <td>478.24</td>\n",
       "      <td>-8.38</td>\n",
       "      <td>25.89</td>\n",
       "      <td>616.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>564.00</td>\n",
       "      <td>504.07</td>\n",
       "      <td>340.30</td>\n",
       "      <td>552.00</td>\n",
       "      <td>1072.88</td>\n",
       "      <td>129.06</td>\n",
       "      <td>41.35</td>\n",
       "      <td>1336.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cc 1 (A)  Maxiumum Current (A)  Residuals (A)  Vcc 1 (V)  \\\n",
       "count  14257.00              14257.00       14257.00   14257.00   \n",
       "mean     146.93                179.01         -32.08     358.16   \n",
       "std      166.79                171.27          70.70     159.06   \n",
       "min        0.00                  0.00        -485.62      20.00   \n",
       "25%        0.00                 25.15         -31.04     366.00   \n",
       "50%       69.00                 94.98         -21.84     432.00   \n",
       "75%      280.00                347.99          -7.33     456.00   \n",
       "max      564.00                504.07         340.30     552.00   \n",
       "\n",
       "       Maxiumum Voltage (V)  Residuals (V)  Amb. Temp (°C)  Irradiance (W/mq)  \n",
       "count              14257.00       14257.00        14257.00           14257.00  \n",
       "mean                 463.22        -105.05           19.50             326.01  \n",
       "std                   20.65         164.62            8.08             333.67  \n",
       "min                  315.94        -614.88           -0.13               4.00  \n",
       "25%                  451.32        -110.29           13.29              33.00  \n",
       "50%                  473.99         -21.13           18.62             175.00  \n",
       "75%                  478.24          -8.38           25.89             616.00  \n",
       "max                 1072.88         129.06           41.35            1336.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------- INV4 ---------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "The timetamps are now used as index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cc 1 (A)</th>\n",
       "      <th>Maxiumum Current (A)</th>\n",
       "      <th>Residuals (A)</th>\n",
       "      <th>Vcc 1 (V)</th>\n",
       "      <th>Maxiumum Voltage (V)</th>\n",
       "      <th>Residuals (V)</th>\n",
       "      <th>Amb. Temp (°C)</th>\n",
       "      <th>Irradiance (W/mq)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14212.00</td>\n",
       "      <td>14212.00</td>\n",
       "      <td>14212.00</td>\n",
       "      <td>14212.00</td>\n",
       "      <td>14212.00</td>\n",
       "      <td>14212.00</td>\n",
       "      <td>14212.00</td>\n",
       "      <td>14212.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.35</td>\n",
       "      <td>179.82</td>\n",
       "      <td>-32.48</td>\n",
       "      <td>368.15</td>\n",
       "      <td>463.12</td>\n",
       "      <td>-94.97</td>\n",
       "      <td>19.52</td>\n",
       "      <td>327.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>166.50</td>\n",
       "      <td>171.69</td>\n",
       "      <td>70.12</td>\n",
       "      <td>163.09</td>\n",
       "      <td>20.67</td>\n",
       "      <td>168.21</td>\n",
       "      <td>8.09</td>\n",
       "      <td>334.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-488.38</td>\n",
       "      <td>18.00</td>\n",
       "      <td>315.94</td>\n",
       "      <td>-585.88</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>25.09</td>\n",
       "      <td>-30.37</td>\n",
       "      <td>382.00</td>\n",
       "      <td>451.18</td>\n",
       "      <td>-89.95</td>\n",
       "      <td>13.30</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.00</td>\n",
       "      <td>95.75</td>\n",
       "      <td>-21.79</td>\n",
       "      <td>440.00</td>\n",
       "      <td>473.85</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>18.65</td>\n",
       "      <td>176.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>284.00</td>\n",
       "      <td>350.04</td>\n",
       "      <td>-7.57</td>\n",
       "      <td>472.00</td>\n",
       "      <td>478.24</td>\n",
       "      <td>3.22</td>\n",
       "      <td>25.93</td>\n",
       "      <td>621.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>547.00</td>\n",
       "      <td>504.07</td>\n",
       "      <td>340.30</td>\n",
       "      <td>561.00</td>\n",
       "      <td>1072.88</td>\n",
       "      <td>127.43</td>\n",
       "      <td>41.35</td>\n",
       "      <td>1336.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cc 1 (A)  Maxiumum Current (A)  Residuals (A)  Vcc 1 (V)  \\\n",
       "count  14212.00              14212.00       14212.00   14212.00   \n",
       "mean     147.35                179.82         -32.48     368.15   \n",
       "std      166.50                171.69          70.12     163.09   \n",
       "min        0.00                  0.00        -488.38      18.00   \n",
       "25%        0.00                 25.09         -30.37     382.00   \n",
       "50%       70.00                 95.75         -21.79     440.00   \n",
       "75%      284.00                350.04          -7.57     472.00   \n",
       "max      547.00                504.07         340.30     561.00   \n",
       "\n",
       "       Maxiumum Voltage (V)  Residuals (V)  Amb. Temp (°C)  Irradiance (W/mq)  \n",
       "count              14212.00       14212.00        14212.00           14212.00  \n",
       "mean                 463.12         -94.97           19.52             327.62  \n",
       "std                   20.67         168.21            8.09             334.58  \n",
       "min                  315.94        -585.88           -0.13               4.00  \n",
       "25%                  451.18         -89.95           13.30              33.00  \n",
       "50%                  473.85         -10.81           18.65             176.00  \n",
       "75%                  478.24           3.22           25.93             621.00  \n",
       "max                 1072.88         127.43           41.35            1336.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for inv_name in inv_names:\n",
    "    print(120 * \"-\" +\"\\n\" + 57 * \"-\", inv_name, 57 * \"-\" + \"\\n\" + 120 * \"-\")\n",
    "    df = inv_data[inv_name]\n",
    "    \n",
    "    if 'Date/Time' in df.columns:\n",
    "        df.index = df['Date/Time']\n",
    "        df.drop(columns = 'Date/Time', inplace=True)\n",
    "        print(\"The timetamps are now used as index\")\n",
    "    \n",
    "    # Visualize some statistics (e.g., means, minimumus, maximums)\n",
    "    display(df[relevant_columns].describe().round(decimals = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#inv_names\n",
    "#inv_data['INV3'].loc[\"2019-11-08\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split (i.e., nominal and abnormal observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Retrieve *failure events* \n",
    "Possible priority values for the *alarm logs*: \n",
    "- \"High\"\n",
    "- \"Medium\"\n",
    "- \"Low\"\n",
    "\n",
    "NOTE: The *(general) faults* will always be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fault_priorities = [\"High\"] #\"Medium\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1) Select which data to load\n",
    "- **Alarm** logs (True/False)\n",
    "- **String Box alarm** logs (True/False)\n",
    "- **Included anonimous faults**: (True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_load = {\n",
    "    'faults': True, \n",
    "    'inv_alarms': False, \n",
    "    'stringBox_alarms': True\n",
    "}\n",
    "include_faults_notRelatedToInverters = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2) Indentify failure events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t\tFAULTS: Soleto 1\n",
      "\t\t\t\tPRIORITIES: High, Medium & faults\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0) 1 fault(s) called 'scheda di comunicazione' have/has been discarted. As it's not related to inverter operation.\n",
      "\n",
      "0) SELECTING only the string box-related faults\n",
      "\n",
      "--> A) General faults loaded (1)\n",
      "--> [B) Inverter logs have been skipped (0)]\n",
      "--> C) String-box logs loaded (INV1: 31230, INV2: 3055, INV3: 5075, INV4: 1615)\n",
      "\n",
      "Loading completed!\n",
      "\n",
      "FAUL CAUSES (4):\n",
      "--------------------\n",
      "1) Allarme string-box\n",
      "2) Ritardo comunicazione dispositivo\n",
      "3) String-box con produzione anomala\n",
      "4) Unknown\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t\t\tFAUL EVENTS (period >= 2018-08-08)\n",
      "\t\t\t\t\tPRIORITIES: Log_stringBox - Medium, Log_stringBox - High, General Fault\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inverter</th>\n",
       "      <th>Componente Guasto</th>\n",
       "      <th>Causa Guasto</th>\n",
       "      <th>Inizio</th>\n",
       "      <th>Fine</th>\n",
       "      <th>Tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CSP3.6 V130086: String-box con produzione anomala</td>\n",
       "      <td>String-box con produzione anomala</td>\n",
       "      <td>2018-08-08 11:51:00</td>\n",
       "      <td>2018-10-18 14:50:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>CSP1.6 V180544 s1: [3] Corrente di stringa fuo...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2018-08-08 13:26:00</td>\n",
       "      <td>2018-08-08 14:52:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>CSP2.6 V180556 s9: [3] Corrente di stringa fuo...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2018-08-08 14:13:00</td>\n",
       "      <td>2018-08-08 14:21:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>CSP2.6 V180556 s9: [3] Corrente di stringa fuo...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2018-08-08 14:33:00</td>\n",
       "      <td>2018-08-08 15:26:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>CSP1.6 V180544 s10: [3] Corrente di stringa fu...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2018-08-08 14:41:00</td>\n",
       "      <td>2018-08-08 14:52:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39915</th>\n",
       "      <td>3</td>\n",
       "      <td>CSP3.6 V130086 s6: [3] Corrente di stringa fuo...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2021-09-16 14:32:00</td>\n",
       "      <td>2021-09-16 14:44:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39916</th>\n",
       "      <td>3</td>\n",
       "      <td>CSP3.6 V130086 s6: [3] Corrente di stringa fuo...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2021-09-16 15:34:00</td>\n",
       "      <td>2021-09-16 15:40:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39917</th>\n",
       "      <td>3</td>\n",
       "      <td>CSP3.6 V130086 s6: [3] Corrente di stringa fuo...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2021-09-16 15:45:00</td>\n",
       "      <td>2021-09-16 15:51:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39918</th>\n",
       "      <td>4</td>\n",
       "      <td>CSP4.6 V180543 s11: [3] Corrente di stringa fu...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2021-09-17 08:46:00</td>\n",
       "      <td>2021-09-17 08:52:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39919</th>\n",
       "      <td>4</td>\n",
       "      <td>CSP4.6 V180543 s11: [3] Corrente di stringa fu...</td>\n",
       "      <td>Allarme string-box</td>\n",
       "      <td>2021-09-17 09:42:00</td>\n",
       "      <td>2021-09-17 09:49:00</td>\n",
       "      <td>Log_stringBox - Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39917 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Inverter                                  Componente Guasto  \\\n",
       "3            3  CSP3.6 V130086: String-box con produzione anomala   \n",
       "4            1  CSP1.6 V180544 s1: [3] Corrente di stringa fuo...   \n",
       "5            2  CSP2.6 V180556 s9: [3] Corrente di stringa fuo...   \n",
       "6            2  CSP2.6 V180556 s9: [3] Corrente di stringa fuo...   \n",
       "7            1  CSP1.6 V180544 s10: [3] Corrente di stringa fu...   \n",
       "...        ...                                                ...   \n",
       "39915        3  CSP3.6 V130086 s6: [3] Corrente di stringa fuo...   \n",
       "39916        3  CSP3.6 V130086 s6: [3] Corrente di stringa fuo...   \n",
       "39917        3  CSP3.6 V130086 s6: [3] Corrente di stringa fuo...   \n",
       "39918        4  CSP4.6 V180543 s11: [3] Corrente di stringa fu...   \n",
       "39919        4  CSP4.6 V180543 s11: [3] Corrente di stringa fu...   \n",
       "\n",
       "                            Causa Guasto              Inizio  \\\n",
       "3      String-box con produzione anomala 2018-08-08 11:51:00   \n",
       "4                     Allarme string-box 2018-08-08 13:26:00   \n",
       "5                     Allarme string-box 2018-08-08 14:13:00   \n",
       "6                     Allarme string-box 2018-08-08 14:33:00   \n",
       "7                     Allarme string-box 2018-08-08 14:41:00   \n",
       "...                                  ...                 ...   \n",
       "39915                 Allarme string-box 2021-09-16 14:32:00   \n",
       "39916                 Allarme string-box 2021-09-16 15:34:00   \n",
       "39917                 Allarme string-box 2021-09-16 15:45:00   \n",
       "39918                 Allarme string-box 2021-09-17 08:46:00   \n",
       "39919                 Allarme string-box 2021-09-17 09:42:00   \n",
       "\n",
       "                      Fine                    Tipo  \n",
       "3      2018-10-18 14:50:00  Log_stringBox - Medium  \n",
       "4      2018-08-08 14:52:00  Log_stringBox - Medium  \n",
       "5      2018-08-08 14:21:00  Log_stringBox - Medium  \n",
       "6      2018-08-08 15:26:00  Log_stringBox - Medium  \n",
       "7      2018-08-08 14:52:00  Log_stringBox - Medium  \n",
       "...                    ...                     ...  \n",
       "39915  2021-09-16 14:44:00  Log_stringBox - Medium  \n",
       "39916  2021-09-16 15:40:00  Log_stringBox - Medium  \n",
       "39917  2021-09-16 15:51:00  Log_stringBox - Medium  \n",
       "39918  2021-09-17 08:52:00  Log_stringBox - Medium  \n",
       "39919  2021-09-17 09:49:00  Log_stringBox - Medium  \n",
       "\n",
       "[39917 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 39917 failure events\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t\tFAULT EVENTS ('General Fault')\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inverter</th>\n",
       "      <th>Componente Guasto</th>\n",
       "      <th>Causa Guasto</th>\n",
       "      <th>Inizio</th>\n",
       "      <th>Fine</th>\n",
       "      <th>Tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>datexel - scheda PV isolation di un quadro di ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-11-08 08:39:00</td>\n",
       "      <td>2019-11-08 12:30:00</td>\n",
       "      <td>General Fault</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Inverter                                  Componente Guasto  \\\n",
       "11291     <NA>  datexel - scheda PV isolation di un quadro di ...   \n",
       "\n",
       "      Causa Guasto              Inizio                 Fine           Tipo  \n",
       "11291      Unknown 2019-11-08 08:39:00  2019-11-08 12:30:00  General Fault  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the fault dataset: Storico guasti.xlsx (a.k.a., 'General faults') & PV SYSTEM - Storico Allarme.xlsx (a.k.a., Log - X)\n",
    "fault_df = fault_utils.load_faults(system_name, include_faults_notRelatedToInverters, to_load['inv_alarms'], fault_priorities, \n",
    "                                   to_load['stringBox_alarms'], to_load['faults'], verbose = False)\n",
    "\n",
    "# Isolate the fault in the period covered by the dataset\n",
    "start_dates = [inv_data[inv_name].index.tolist()[0] for inv_name in inv_names]\n",
    "first_start_date = sorted(start_dates)[0]\n",
    "fault_df = fault_df[fault_df[\"Inizio\"] >= first_start_date]\n",
    "\n",
    "print(\"\\n\"+\"-\"* 120 + f\"\\n\\t\\t\\t\\t\\t\\tFAUL EVENTS (period >= {first_start_date.strftime('%Y-%m-%d')})\")\n",
    "print(\"\\t\\t\\t\\t\\tPRIORITIES:\", (', ').join([priority for priority in fault_df['Tipo'].unique()]) \n",
    "      + \"\\n\" + \"-\"* 120)\n",
    "display(fault_df)\n",
    "print(f\"TOTAL: {len(fault_df)} failure events\")\n",
    "\n",
    "# --------------- Isolate only the (general) faults observations ---------------------------------\n",
    "fault_to_vis = \"General Fault\" \n",
    "fault_type_cond = fault_df[\"Tipo\"] == fault_to_vis\n",
    "print(\"\\n\"+\"-\"* 105 + f\"\\n\\t\\t\\t\\t\\tFAULT EVENTS ('{fault_to_vis}')\\n\" + \"-\"* 105)\n",
    "only_fault_df = fault_df[fault_type_cond]\n",
    "if len(only_fault_df) > 0:\n",
    "    display(only_fault_df)\n",
    "else:\n",
    "    print(f\"\\n[{system_name}] No faults available for this PV system.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Train and test split\n",
    "1) **'random'**: randomly splitting (by a percentage)\n",
    "\n",
    "2) **'cutting_date'**: splitting according to a cutting date (computed by a percentage)\n",
    "\n",
    "3) **'failure_events'**: splitting according to periods concerning failure events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_split_stategy = 'random' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = dict()\n",
    "test_data = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRAT 1: Split train/test randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size =  0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------- INV1 --------------\n",
      "TRAIN SET (60 %): 8566 obs. \n",
      " TEST SET (40 %): 5711 obs.\n",
      "\n",
      "-------------- INV2 --------------\n",
      "TRAIN SET (60 %): 8505 obs. \n",
      " TEST SET (40 %): 5670 obs.\n",
      "\n",
      "-------------- INV3 --------------\n",
      "TRAIN SET (60 %): 8554 obs. \n",
      " TEST SET (40 %): 5703 obs.\n",
      "\n",
      "-------------- INV4 --------------\n",
      "TRAIN SET (60 %): 8527 obs. \n",
      " TEST SET (40 %): 5685 obs.\n"
     ]
    }
   ],
   "source": [
    "if train_test_split_stategy == 'random':\n",
    "    for inv_name in inv_names:\n",
    "        print(\"\\n\"+14*\"-\", inv_name, 14*\"-\")\n",
    "        df = inv_data[inv_name]\n",
    "        \n",
    "        train_df, test_df = train_test_split(df, test_size = test_size, random_state = 101)\n",
    "        train_data[inv_name] = train_df\n",
    "        test_data[inv_name] = test_df\n",
    "        \n",
    "        print(f\"TRAIN SET ({int((1- test_size)*100)} %): {len(train_df)} obs. \\n \"\\\n",
    "              f\"TEST SET ({int(test_size * 100)} %): {len(test_df)} obs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRAT 2: Simple train/test split (based on the number of days available) \n",
    "\n",
    "**STAT 1**: *simple_train_test_split* = **True**\n",
    "\n",
    "- **TRAIN**: [starting day : cutoff_date]\n",
    "\n",
    "- **TEST**: [cutoff_date + 1 : ending date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size =  0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This splitting stategy has not been selected.\n"
     ]
    }
   ],
   "source": [
    "if train_test_split_stategy == 'cutting_date':\n",
    "    for inv_name in inv_names:\n",
    "        print(\"\\n\"+30*\"-\", inv_name, 30*\"-\")\n",
    "        df = inv_data[inv_name]\n",
    "        \n",
    "        # Define the cutoff date\n",
    "        starting_date = pd.to_datetime(df.index[0])\n",
    "        ending_date = pd.to_datetime(df.index[-1])\n",
    "        days_available = (ending_date - starting_date).components[0]\n",
    "        num_test_days = int(round(days_available * test_size, 0))\n",
    "        cutoff_date = ending_date - pd.Timedelta(num_test_days, unit=\"days\")\n",
    "\n",
    "        # Create the subsets\n",
    "        train_data[inv_name] = df[df.index <= cutoff_date]\n",
    "        test_data[inv_name] = df[df.index > cutoff_date]\n",
    "\n",
    "        print(f\"DAYS AVAILABLE: {days_available} {starting_date.strftime('%Y-%m'), ending_date.strftime('%Y-%m')}\"\\\n",
    "              f\"\\n--> TRAIN: {days_available - num_test_days} days || TEST ({int(test_size*100)} %): {num_test_days} days \"\\\n",
    "              f\"\\n--> Cut-off date: {cutoff_date.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(\"This splitting stategy has not been selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRAT 3: Split the data according to the faults/allarm logs\n",
    "**STAT 2**: *simple_train_test_split* = **False**\n",
    "- **TEST**: [Faults/allarm logs]\n",
    "- **TRAIN**: [not test observations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fault_priorities = [\"High\"] #\"Medium\"\n",
    "days_to_include = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_validation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cutting-edge splitting stategy has not been selected. Sorry :/\n"
     ]
    }
   ],
   "source": [
    "valid_data = dict()\n",
    "if train_test_split_stategy == 'failure_events':\n",
    "    dimensions = []\n",
    "    \n",
    "    for inv_name in inv_names:\n",
    "        print(\"\\n\" + 25 * \"-\", inv_name, 25*\"-\")\n",
    "        df = inv_data[inv_name]\n",
    "\n",
    "        # Carry out the split according to the faults available \n",
    "        train_idk, test_idk = fault_utils.train_test_split(fault_df, df, inv_name,\n",
    "                                                           priorities_to_consider = test_fault_priorities,\n",
    "                                                           time_window = days_to_include, verbose = False)\n",
    "\n",
    "        # Save the train/test dataframe for each inverter\n",
    "        train_data[inv_name] = df.loc[train_idk, :]\n",
    "        test_data[inv_name] = df.loc[test_idk, :]\n",
    "        \n",
    "        print(f\"TRAIN SET SIZE: {len(train_data[inv_name])} hourly observations \"\\\n",
    "              f\"({round((len(train_data[inv_name])/len(df))*100, 2)} %)\")\n",
    "        \n",
    "        # Generate the test/validation split \n",
    "        if generate_validation:\n",
    "            valid_dimension = 0.5\n",
    "            val_set, test_set, tot_periods = utils.split_test_validation_sets(test_data[inv_name], valid_dimension, \n",
    "                                                                              verbose = True)\n",
    "            # Set the validation/test sets\n",
    "            valid_data[inv_name] = val_set\n",
    "            test_data[inv_name] = test_set\n",
    "            print(f\"VALID SET SIZE: {' ' if len(train_idk) >= 10000 else ''}\"\\\n",
    "                  f\"{len(valid_data[inv_name])} hourly observations ({round((len(valid_data[inv_name])/len(df))*100 , 2)} %)\")\n",
    "            \n",
    "            # Save the dimensions of the split for each inverter\n",
    "            dimensions.append((len(df), len(train_data[inv_name]), len(test_data[inv_name]), len(valid_data[inv_name])))\n",
    "        else:\n",
    "            # Save the dimensions of the split for each inverter\n",
    "            dimensions.append((len(df), len(train_data[inv_name]), len(test_data[inv_name])))\n",
    "            \n",
    "        print(f\" TEST SET SIZE: {' 'if len(train_idk) >= 10000 else ''}{len(test_data[inv_name])} hourly observations \"\\\n",
    "              f\"({round((len(test_data[inv_name])/len(df))*100, 2)} %)\")\n",
    "        \n",
    "        #print(\"[!!] CHECK:\", (len(train_data[inv_name]) + len(valid_data[inv_name]) + len(test_data[inv_name]))/len(df))\n",
    "        \n",
    "    # Visualize the average set dimensions\n",
    "    avg_train_dim = np.mean([dim[1]/dim[0] for dim in dimensions])\n",
    "    avg_test_dim = np.mean([dim[2]/dim[0] for dim in dimensions])\n",
    "    if generate_validation:\n",
    "        avg_val_dim = np.mean([dim[3]/dim[0] for dim in dimensions])\n",
    "        \n",
    "    print(\"\\n\\t\\t\" + \"-\" * 26 + f\"\\n\\t\\t TRAIN SET (AVG): {round(avg_train_dim * 100, 1)} % \\n\\t\\t \"\n",
    "          + (f\"VALID SET (AVG): {round(avg_val_dim * 100, 1)} % \\n\\t\\t  \" if generate_validation else ' ')\n",
    "          + f\"TEST SET (AVG): {round(avg_test_dim * 100, 1)} %\" + \"\\n\\t\\t\"+ \"-\" * 26)\n",
    "else:\n",
    "    print(\"This cutting-edge splitting stategy has not been selected. Sorry :/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Retrieve the observations concerning fault events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.0) Load the pre-computed failure events for each inverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_failure_events = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving_folder_name = \"UC2 - oneClassSVM\"\n",
    "saving_folder_path = path.join(system_path, \"..\", \"..\", saving_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_names = ['fault_events', 'unique_faults']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inv_failure_events = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading: False\n"
     ]
    }
   ],
   "source": [
    "if load_failure_events:\n",
    "    dataset_name = 'train'\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        for inv_name in inv_names:\n",
    "            print(f'[{dataset_name}] {inv_name}: {file_name}')\n",
    "                \n",
    "            # File path\n",
    "            loading_path = path.join(saving_folder_path, f'{inv_name}_{dataset_name}_{file_name}.csv')\n",
    "            \n",
    "            # Load data\n",
    "            loaded_data = pd.read_csv(loading_path, header=None, index_col=0, dtype = pd.array).squeeze().to_dict()\n",
    "            display(loaded_data.info())\n",
    "            \n",
    "            # Save the data \n",
    "            train_inv_failure_events[inv_name].append(loaded_data)\n",
    "            \n",
    "        print('-' * 20)\n",
    "else:\n",
    "    print(\"Pre-loading: False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.1) Retrieve the failure events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9010/3920524670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_failure_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtimestamp_faults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         fault_events, unique_faults = fault_utils.find_fault_observation(fault_df, train_data[inv_name], inv_name, \n\u001b[0m\u001b[1;32m      5\u001b[0m                                                                         \u001b[0minclude_faults_notRelatedToInverters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                         verbose = False)\n",
      "\u001b[0;32m~/notebooks/sample/_library/fault_utils.py\u001b[0m in \u001b[0;36mfind_fault_observation\u001b[0;34m(fault_df, inv_df, inv_name, include_faults_notRelatedToInverters, tolerance, verbose)\u001b[0m\n\u001b[1;32m    381\u001b[0m      \u001b[0;31m# Extraxt the Starting and ending timestamps (save them as a Python list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0minv_faults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Period\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_faults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Inizio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fine\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0minv_faults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Period\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_faults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Period\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_fault_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0minv_faults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_faults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Inizio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fine\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sample/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4428\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4429\u001b[0m         \"\"\"\n\u001b[0;32m-> 4430\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/anaconda3/envs/sample/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sample/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1138\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sample/lib/python3.8/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/notebooks/sample/_library/fault_utils.py\u001b[0m in \u001b[0;36mextract_fault_period\u001b[0;34m(merged_col)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# ENDING TEMESTAMP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0minferred_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sample/lib/python3.8/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1076\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;31m#  error: Incompatible return value type (got \"Union[Timestamp, NaTType,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sample/lib/python3.8/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0morig_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"coerce\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sample/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mmaybe_convert_dtype\u001b[0;34m(data, copy)\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2266\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2267\u001b[0m         \u001b[0;31m# Note: we must cast to datetime64[ns] here in order to treat these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m         \u001b[0;31m#  as wall-times instead of UTC timestamps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for inv_name in inv_names:\n",
    "    if not load_failure_events:\n",
    "        timestamp_faults = dict()\n",
    "        fault_events, unique_faults = fault_utils.find_fault_observation(fault_df, train_data[inv_name], inv_name, \n",
    "                                                                        include_faults_notRelatedToInverters = False, \n",
    "                                                                        verbose = False)\n",
    "\n",
    "        ts_to_remove = sorted(fault_events.keys())\n",
    "        fault_types = set(fault_events[0][0] for ts, fault_events in fault_events.items())\n",
    "        print(f\"{inv_name.upper()} (TRAIN DATA): found {len(unique_faults)} unique fault events ({len(ts_to_remove)} obs.)\",\n",
    "              f\"\\n\\t--> Faults/allarms types ({str(len(fault_types))}): {(', ').join(fault_types)}\\n\" \n",
    "              if not verbose and len(fault_types) > 0 else \"--> Nice, that's perfect!\")\n",
    "        timestamp_faults[inv_name] = ts_to_remove\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\t\"+ 80 * \"-\")\n",
    "            for ts, fault_events in fault_events.items():\n",
    "                print(\"\\t\", pd.to_datetime(ts).strftime('%Y-%m-%d (%H:%M)'))\n",
    "                for fault_event in fault_events:\n",
    "                    print(\"\\t --> \" + fault_event[0].upper() + \" (\"+fault_event[1] +\") \"+ fault_event[2][:80] +  \n",
    "                          \"\\n\\t\\t\\t [\" +  fault_event[3][0].strftime('%Y-%m-%d (%H:%M)') + \" -  \" + \n",
    "                          fault_event[3][1].strftime('%Y-%m-%d (%H:%M)') + \"]\\n\")\n",
    "\n",
    "        train_inv_failure_events[inv_name] = (fault_events, unique_faults)\n",
    "    else:\n",
    "        failure_events_ts = train_inv_failure_events[inv_name][0].keys()\n",
    "        timestamp_faults[inv_name] = failure_events_ts\n",
    "        \n",
    "        print(f\"{inv_name} {len(failure_events_ts)} Failure events have been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.2) Drop faulty observations from the train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_obs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if drop_obs:\n",
    "    for inv_name in inv_names:\n",
    "        idk_to_drop = timestamp_faults[inv_name]\n",
    "\n",
    "        if len(idk_to_drop) > 0:\n",
    "            train_data[inv_name] = train_data[inv_name].drop(index = idk_to_drop)\n",
    "            print(f\"{inv_name}: {len(idk_to_drop)} timestamps ({round(len(idk_to_drop)/len(train_data[inv_name])*100, 2)} %) \"\\\n",
    "                  f\"concerning fault events have been removed from the TRAIN DATA.\")\n",
    "        else:\n",
    "            print(f\"{inv_name}: Nice, everything is okay :) --> No fault events have been found in the TRAIN data.\")\n",
    "else:\n",
    "    print(\"This step will be skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.2) Save the data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the saving folder\n",
    "if not path.exists(saving_folder_path):\n",
    "    makedirs(saving_folder_path) \n",
    "    print(f\"PV System --> {system_name.upper()}\\nA new saving folder has been created: {saving_folder_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not load_failure_events:\n",
    "    for inv_name in inv_names: \n",
    "        print('-' * 20, inv_name, '-' * 20)\n",
    "        \n",
    "        dataset_name = 'train'\n",
    "        \n",
    "        # Paths \n",
    "        file_path_a = path.join(saving_folder_path, f'{inv_name}_{dataset_name}_{file_names[0]}.csv')\n",
    "        file_path_b = path.join(saving_folder_path, f'{inv_name}_{dataset_name}_{file_names[1]}.csv')\n",
    "\n",
    "        # CSV Writers\n",
    "        fault_events_writer = csv.writer(open(file_path_a, \"w\"))\n",
    "        unique_faults_writer = csv.writer(open(file_path_b, \"w\"))\n",
    "\n",
    "        # Failure items\n",
    "        fault_events, unique_faults = train_inv_failure_events[inv_name]\n",
    "        \n",
    "        # Item 1\n",
    "        for key, val in fault_events.items():\n",
    "            fault_events_writer.writerow([key, val])\n",
    "        print(f\"a) [{dataset_name.upper()} DATA] Fault events have been saved\")\n",
    "            \n",
    "        # Item 2\n",
    "        for failure_event in unique_faults:\n",
    "            unique_faults_writer.writerow(failure_event)\n",
    "        print(f\"b) [{dataset_name.upper()} DATA] All the unique fault events have been saved\\n\")\n",
    "else:\n",
    "    print(\"Pre-loading: False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b ) [Test data] Check *whether* and *how many* fault events are included in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.0) Load the pre-computed failure events for each inverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_inv_failure_events = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if load_failure_events:\n",
    "    dataset_name = 'test'\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        for inv_name in inv_names:\n",
    "            print(f'[{dataset_name}] {inv_name}: {file_name}')\n",
    "                \n",
    "            # File path\n",
    "            loading_path = path.join(saving_folder_path, f'{inv_name}_{dataset_name}_{file_name}.csv')\n",
    "            \n",
    "            # Load data\n",
    "            loaded_data = pd.read_csv(loading_path, header=None, index_col=0, dtype = pd.array).squeeze().to_dict()\n",
    "            display(loaded_data.info())\n",
    "            \n",
    "            # Save the data \n",
    "            train_inv_failure_events[inv_name].append(loaded_data)\n",
    "            \n",
    "        print('-' * 20)\n",
    "else:\n",
    "    print(\"Pre-loading: False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.1) Compute the failure events (for the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not load_failure_events:\n",
    "    print(\"-\"*40, \"TEST DATA\", \"-\"*40)\n",
    "    test_fault_events = dict()\n",
    "    test_inv_failure_events = dict()\n",
    "    for inv_name in inv_names:\n",
    "        fault_events, unique_faults = fault_utils.find_fault_observation(fault_df, test_data[inv_name], inv_name, verbose=False)\n",
    "        test_fault_events[inv_name] = fault_events\n",
    "        test_inv_failure_events[inv_name] = (fault_events, unique_faults)\n",
    "\n",
    "        if len(test_fault_events[inv_name]) > 1:\n",
    "            print(f\"{inv_name} --> OK: {len(test_fault_events[inv_name])} timestamps \"\\\n",
    "                  f\"({round(len(test_fault_events[inv_name])/len(test_data[inv_name])*100, 2)} %) concerning \"\\\n",
    "                  f\"{len(unique_faults)} fault events have been found in the test set.\")\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\\t\"+ 85 * \"-\")\n",
    "                for idk, fault_event in enumerate(unique_faults):\n",
    "                    print(f\"\\t--> ({idk +1 }/{len(unique_faults)}) \" + fault_event[0].upper() + \" (\"+fault_event[1] +\") \"+ \n",
    "                          fault_event[2][:65] +  \"...\\n\\t\\t\\t [\" +  pd.to_datetime(fault_event[3][0]).strftime('%Y-%m-%d (%H:%M)') \n",
    "                          + \" -  \" + pd.to_datetime(fault_event[3][1]).strftime('%Y-%m-%d (%H:%M)') + \"]\\n\")            \n",
    "        else:\n",
    "            print(f\"{inv_name} (ISSUE): No fault events have been found in the test data \"\\\n",
    "                  f\"(TOTAL: {len(test_data[inv_name])} obs.)\")\n",
    "else:\n",
    "    failure_events_ts = test_inv_failure_events[inv_name][0].keys()\n",
    "    test_fault_events[inv_name] = failure_events_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.2) Save the failure events (for the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not load_failure_events:\n",
    "    for inv_name in inv_names: \n",
    "        print('-' * 20, inv_name, '-' * 20)\n",
    "        \n",
    "        dataset_name = 'test'\n",
    "        \n",
    "        # Paths \n",
    "        file_path_a = path.join(saving_folder_path, f'{inv_name}_{dataset_name}_{file_names[0]}.csv')\n",
    "        file_path_b = path.join(saving_folder_path, f'{inv_name}_{dataset_name}_{file_names[1]}.csv')\n",
    "\n",
    "        # CSV Writers\n",
    "        fault_events_writer = csv.writer(open(file_path_a, \"w\"))\n",
    "        unique_faults_writer = csv.writer(open(file_path_b, \"w\"))\n",
    "\n",
    "        # Failure items\n",
    "        fault_events, unique_faults = test_inv_failure_events[inv_name]\n",
    "        #print(unique_faults)\n",
    "        \n",
    "        # Item 1\n",
    "        for key, val in fault_events.items():\n",
    "            fault_events_writer.writerow([key, val])\n",
    "        print(f\"a) [{dataset_name.upper()} DATA] Fault events have been saved\")\n",
    "            \n",
    "        # Item 2\n",
    "        for failure_event in unique_faults:\n",
    "            unique_faults_writer.writerow(failure_event)\n",
    "        print(f\"b) [{dataset_name.upper()} DATA] All the unique fault events have been saved\\n\")\n",
    "else:\n",
    "    print(\"Pre-loading: False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_cases = [\n",
    "    # ----- 0 -----------------------\n",
    "    ['Residuals (A)', 'Residuals (V)'],\n",
    "    # ----- 1 -------\n",
    "    ['Residuals (A)'],\n",
    "    # ----- 2 ------------------------\n",
    "    ['Cc 1 (A)', 'Residuals (A)', 'Vcc 1 (V)', 'Residuals (V)', 'Amb. Temp (°C)', 'Cell Temp (°C)', 'Irradiance (W/mq)']\n",
    "]\n",
    "input_columns = input_cases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inv_name = 'INV3'\n",
    "#test_data[inv_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for inv_name in inv_names:\n",
    "    print(120 * \"-\" +\"\\n\" + 57 * \"-\", inv_name, 57 * \"-\" + \"\\n\" + 120 * \"-\")\n",
    "    \n",
    "    # Dataset\n",
    "    df = inv_data[inv_name]\n",
    "    train_obs = train_data[inv_name]\n",
    "    test_obs = test_data[inv_name]\n",
    "    \n",
    "    # 0) Input for the SVM\n",
    "    input_train_data = train_obs[input_columns]\n",
    "    input_test_data = test_obs[input_columns]\n",
    "    print(\"\\n\" + \"-\" * 40, \"FITTING THE (ONE-CLASS) SVM\", \"-\" * 40)\n",
    "    print(f\"FEATURES ({len(input_columns)}):\", ' || '.join(input_columns), \"\\n\")\n",
    "    print(f\"TRAIN DATA: {len(input_train_data)} observations \"\\\n",
    "          f\"({round((len(input_train_data)/len(df))*100, 2)} %)\")\n",
    "    print(f\"TEST  DATA: {len(input_test_data)} observations \"\\\n",
    "          f\"({round((len(input_test_data)/len(df))*100, 2)} %)\\n\")\n",
    "        \n",
    "    # 1) One-class Support Vector Machine (SVM)\n",
    "    print(f\"--> Fitting the 'one-class SVM' on the residuals of {len(input_train_data)} train observations...\")\n",
    "    \n",
    "    # 1.1) Input Data\n",
    "    input_svm_data = input_train_data\n",
    "    \n",
    "    # 1.2) Kernel \n",
    "    # -------------------- 0 ------ 1 ----- 2 ------ 3 --- \n",
    "    possible_kernels = ['linear', 'poly', 'rbf', 'sigmoid'] \n",
    "    svm_kernel = possible_kernels[3]\n",
    "    poly_degree = 3\n",
    "    \n",
    "    print(\"--> KERNEL:\", svm_kernel)\n",
    "    if svm_kernel == possible_kernels[1]:\n",
    "        print(f\"--> Polynomial degree:\", poly_degree)\n",
    "    \n",
    "    # 1.3) Choose the NU parameter: it controls the sensitivity of the support vectors \n",
    "    # --> it should be tuned to the approximate ratio of outliers in the data (e.g. 0.01%)\n",
    "    num_failure_events = len(timestamp_faults[inv_name])\n",
    "    support_vectors_sensitivity =  num_failure_events / len(input_svm_data) \n",
    "    if support_vectors_sensitivity == 0:\n",
    "        support_vectors_sensitivity = 1 * (10 ** -5)\n",
    "    print(f\"--> Support vector sensitivity: {np.round(support_vectors_sensitivity, 5)} \"\\\n",
    "          f\"({num_failure_events} obs concerning failure events)\\n\")\n",
    "\n",
    "    # 1.4) Define and train the One-Class SVM\n",
    "    svm = OneClassSVM(kernel = svm_kernel, degree = poly_degree, nu = support_vectors_sensitivity)\n",
    "    svm.fit(input_svm_data) \n",
    "    \n",
    "    # 2.a) [TRAIN] Predict the classes\n",
    "    print(\"-\" * 40, \"[TRAIN] PREDICT CLASSES\", \"-\" * 40)\n",
    "    print(f\"--> Predicting the class on the residuals of {len(input_train_data)} train observations...\")\n",
    "    predicted_train_classes = svm.predict(input_train_data)\n",
    "    idk_train_class_a = [value[0] for value in np.argwhere(predicted_train_classes == -1)]\n",
    "    idk_train_class_b = [value[0] for value in np.argwhere(predicted_train_classes == 1)]\n",
    "    print(f\"--> PREDICTED CLASSES for {len(predicted_train_classes)} observations.\")\n",
    "    print(f\"\\t--> CLASS '-1': {len(idk_train_class_a)} obs. \"\\\n",
    "          f\"({round((len(idk_train_class_a)/len(predicted_train_classes))*100, 2)} %)\")\n",
    "    print(f\"\\t--> CLASS '+1': {len(idk_train_class_b)} obs. \"\\\n",
    "          f\"({round((len(idk_train_class_b)/len(predicted_train_classes))*100, 2)} %)\\n\")\n",
    "    \n",
    "    # 2.b) [TEST] Predict the class\n",
    "    print(\"-\" * 40, \"[TEST] PREDICT CLASSES\", \"-\" * 40)\n",
    "    print(f\"--> Predicting the class on the residuals of {len(input_test_data)} test observations...\")\n",
    "    predicted_test_classes = svm.predict(input_test_data)\n",
    "    idk_test_class_a = [value[0] for value in np.argwhere(predicted_test_classes == -1)]\n",
    "    idk_test_class_b = [value[0] for value in np.argwhere(predicted_test_classes == 1)]\n",
    "    print(f\"--> PREDICTED CLASSES for {len(predicted_test_classes)} observations.\")\n",
    "    print(f\"\\t--> CLASS '-1': {len(idk_test_class_a)} obs. \"\\\n",
    "          f\"({round((len(idk_test_class_a)/len(predicted_test_classes))*100, 2)} %)\")\n",
    "    print(f\"\\t--> CLASS '+1': {len(idk_test_class_b)} obs. \"\\\n",
    "          f\"({round((len(idk_test_class_b)/len(predicted_test_classes))*100, 2)} %)\\n\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 30, \"CLASS '-1'\", \"-\" * 30)\n",
    "        display(sorted(set(test_obs.loc[test_obs.index[idk_test_class_a], :].index.strftime('%Y-%m-%d')))) # (%H:%M)\n",
    "        print(\"\\n\"+ \"-\" * 30, \"CLASS '1'\", \"-\" * 30)\n",
    "        display(sorted(set(test_obs.loc[test_obs.index[idk_test_class_b], :].index.strftime('%Y-%m-%d'))))\n",
    "    \n",
    "    # 2.c) [ALL DATA] Predict the class\n",
    "    input_data = df[input_columns]\n",
    "    print(\"-\" * 38, \"[ALL DATA] PREDICT CLASSES\", \"-\" * 38)\n",
    "    print(f\"--> Predicting the class on the residuals of {len(input_data)} all the observations...\")\n",
    "    predicted_classes = svm.predict(input_data)\n",
    "    idk_class_a = [value[0] for value in np.argwhere(predicted_classes == -1)]\n",
    "    idk_class_b = [value[0] for value in np.argwhere(predicted_classes == 1)]\n",
    "    print(f\"--> PREDICTED CLASSES for {len(predicted_classes)} observations.\")\n",
    "    print(f\"\\t--> CLASS '-1': {len(idk_class_a)} obs. \"\\\n",
    "          f\"({round((len(idk_class_a)/len(predicted_classes))*100, 2)} %)\")\n",
    "    print(f\"\\t--> CLASS '+1': {len(idk_class_b)} obs. \"\\\n",
    "          f\"({round((len(idk_class_b)/len(predicted_classes))*100, 2)} %)\\n\")\n",
    "    \n",
    "    # 3) Create a new column for the predicted class\n",
    "    df.loc[:, 'Predicted class'] = 99\n",
    "    df.loc[:, \"Dataset_type\"] = 99\n",
    "    \n",
    "    # 3.1) Filling the data with the predicted classes\n",
    "    df.loc[df.index[idk_class_a], 'Predicted class'] = -1\n",
    "    df.loc[df.index[idk_class_b], 'Predicted class'] = 1\n",
    "    \n",
    "    # Label the dataset with the dataset type\n",
    "    df.loc[train_obs.index, \"Dataset_type\"] = -10\n",
    "    df.loc[test_obs.index, \"Dataset_type\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Retrieve the failure events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inv_fault_events = dict()\n",
    "for inv_name in inv_names:\n",
    "    df = inv_data[inv_name]\n",
    "    fault_events, unique_faults = fault_utils.find_fault_observation(fault_df, df, inv_name, verbose=False)\n",
    "    \n",
    "    inv_fault_events[inv_name] = (fault_events, unique_faults)\n",
    "    print(inv_name, f\"[TEST DATA]: {len(unique_faults)} unique fault events ({len(fault_events)} obs.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Assign a class depending on the failure timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failure_col_name = 'Failure event'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = {'Nominal obs': 'No', 'Failure events': 'Yes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for inv_name in inv_names:\n",
    "    print(\"-\" * 30, inv_name, \"-\" * 30)\n",
    "    df = inv_data[inv_name]\n",
    "    fault_events, unique_faults = inv_fault_events[inv_name]\n",
    "    \n",
    "    print(f\"TOTAL (All data): {len(df)} obs.\")\n",
    "    print(f\"FAILURE EVENTS: {len(fault_events.keys())} obs.\")\n",
    "    \n",
    "    # Default class\n",
    "    df.loc[:, failure_col_name] = class_names['Nominal obs']\n",
    "    \n",
    "    # Class failure events\n",
    "    failure_ts = fault_events.keys()\n",
    "    df.loc[failure_ts, failure_col_name] = class_names['Failure events']\n",
    "    \n",
    "    # Retrieve the classes \n",
    "    classes = df.groupby(by = failure_col_name).count()['Predicted class'].to_dict()\n",
    "    assert classes['Yes'] == len(failure_ts)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 10, \"CLASSES\", \"-\" * 10)\n",
    "    print(classes, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the outcomes visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_col = 'Cell Temp (°C)' #'Amb. Temp (°C)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_types = ['train', 'test', 'All']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_to_visualize = dataset_types[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for inv_name in inv_names:\n",
    "    print(120 * \"-\" +\"\\n\" + 57 * \"-\", inv_name, 57 * \"-\" + \"\\n\" + 120 * \"-\")\n",
    "    \n",
    "    # Retrieve the main dataset\n",
    "    data = inv_data[inv_name]\n",
    "    \n",
    "    if data_to_visualize == 'test':\n",
    "        test_ts = test_data[inv_name].index\n",
    "        test_df = data.loc[test_ts, :]\n",
    "        df = test_df\n",
    "    elif data_to_visualize == 'train':\n",
    "        train_ts = train_data[inv_name].index\n",
    "        train_df = data.loc[test_ts, :]\n",
    "        df = train_df\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    # 0) Create the inputs data\n",
    "    amb_conditions = np.array(list(zip(df[temp_col].values, df['Irradiance (W/mq)'].values)))\n",
    "    voltage_values = np.array(df['Vcc 1 (V)'].values)\n",
    "    current_values = np.array(df['Cc 1 (A)'].values)\n",
    "    max_voltage = np.array(df['Maxiumum Voltage (V)'].values)\n",
    "    max_current = np.array(df['Maxiumum Current (A)'].values)\n",
    "    \n",
    "    # Visualize both the predicted classes and the residuals\n",
    "    title_types = [\n",
    "        f\"({data_to_visualize.upper()}) Classification (A)\", \n",
    "        f'({data_to_visualize.upper()}) Actual failure events (B)', \n",
    "        f\"({data_to_visualize.upper()}) Residuals (C)\", \n",
    "        f\"({data_to_visualize.upper()}) Dataset type (D)\"\n",
    "    ]\n",
    "    \n",
    "    hue_types = [\n",
    "            df['Predicted class'].values, \n",
    "            df[failure_col_name].map({'No': 99, 'Yes': 101}).values,\n",
    "            (df['Residuals (V)'].values, df['Residuals (A)'].values), \n",
    "            df['Dataset_type'].values\n",
    "        ]\n",
    "    for idk, hue_values in enumerate(hue_types):\n",
    "        if data_to_visualize != 'All' and idk == 3:\n",
    "                continue\n",
    "        \n",
    "        # 1) Create the visual panel\n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        fig.suptitle(f\"[{inv_name}] {title_types[idk]}\", size = 40, y = 0.8)\n",
    "\n",
    "        # 1.1) [VOLTAGE] Generate the 3-dimensional subplot\n",
    "        #interpolation_utils.\n",
    "        voltage_hue_values = hue_values[0] if isinstance(hue_values, tuple) else hue_values\n",
    "        interpolation_utils.generate_sub_graph(fig, 0, amb_conditions, voltage_values, max_voltage, 'Voltage (V)', \n",
    "                                               pov_elev = 20, pov_angle = -140, visualize_actual_points = True,\n",
    "                                               visualize_surface = False,  hue_values = voltage_hue_values)\n",
    "\n",
    "        # 1.2) [CURRENT] Generate the 3-dimensional subplot\n",
    "        current_hue_values = hue_values[1] if isinstance(hue_values, tuple) else hue_values\n",
    "        interpolation_utils.generate_sub_graph(fig, 1, amb_conditions, current_values, max_current, 'Current (A)', \n",
    "                                               pov_elev = 20, pov_angle = -140, visualize_actual_points = True, \n",
    "                                               visualize_surface = False, hue_values = current_hue_values)\n",
    "\n",
    "        # Visualize the graphical panel\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compute TP, FP, TN & FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_types = dict()\n",
    "for selected_dataset in dataset_types:\n",
    "    print(\"\\n\" + \"-\" * 104, \"\\n\" + \"-\" * 50 + selected_dataset.upper() + \"-\" * 50 + \"\\n\" + \"-\" * 104)\n",
    "    \n",
    "    inv_metrics = dict()\n",
    "    for inv_name in inv_names:\n",
    "        print(\"-\" * 30, inv_name, \"-\" * 30)\n",
    "\n",
    "        # Retrieve the main dataset\n",
    "        data = inv_data[inv_name]\n",
    "        if selected_dataset == 'test':\n",
    "            test_ts = test_data[inv_name].index\n",
    "            test_df = data.loc[test_ts, :]\n",
    "            df = test_df\n",
    "        elif selected_dataset == 'train':\n",
    "            train_ts = train_data[inv_name].index\n",
    "            train_df = data.loc[train_ts, :]\n",
    "            df = train_df\n",
    "        else:\n",
    "            df = data\n",
    "        print(f\"TOTAL ({selected_dataset}): {len(df)} obs.\")\n",
    "\n",
    "        # Retrieve the failure events\n",
    "        fault_events, unique_faults = inv_fault_events[inv_name]\n",
    "\n",
    "        # Label the dataset\n",
    "        failure_col_name = 'Failure event'\n",
    "        df.loc[:, failure_col_name] = \"No\"\n",
    "\n",
    "        failure_ts = [ts for ts in fault_events.keys() if ts in df.index]\n",
    "        print(f\"FAILURE EVENTS: {len(failure_ts)} obs.\")\n",
    "        df.loc[failure_ts, failure_col_name] = \"Yes\"\n",
    "\n",
    "        # Compute the metrics\n",
    "        col_name = 'metrics'\n",
    "        df.loc[:, col_name] = \"Unknown\"\n",
    "\n",
    "        # FUNCTIONS: True/False POSTIVE \n",
    "        tp_cond = (df[failure_col_name] == \"Yes\") & (df['Predicted class'] == -1)\n",
    "        fp_cond = (df[failure_col_name] == \"No\") & (df['Predicted class'] == -1)\n",
    "\n",
    "        # FUNCTIONS: True/False NEGATIVE \n",
    "        tn_cond = (df[failure_col_name] == \"No\") & (df['Predicted class'] == 1)\n",
    "        fn_cond = (df[failure_col_name] == \"Yes\") & (df['Predicted class'] == 1)\n",
    "\n",
    "        # COMPUTE the metrics \n",
    "        df.loc[tp_cond, col_name] = \"TP\"\n",
    "        df.loc[fp_cond, col_name] = \"FP\"\n",
    "        df.loc[tn_cond, col_name] = \"TN\"\n",
    "        df.loc[fn_cond, col_name] = \"FN\"\n",
    "\n",
    "        # Retrieve the values for the metrics\n",
    "        grouped_df = df.groupby(by = col_name).count()['Predicted class']\n",
    "        raw_metrics = grouped_df.to_dict()\n",
    "\n",
    "        # Add potential missing metrics (due to missing cases) --> e.g., TP may be missing\n",
    "        metrics_names = ['TP', 'FP', 'TN', 'FN']\n",
    "        missing_metrics = [metrics_name for metrics_name in metrics_names if metrics_name not in list(raw_metrics.keys())]\n",
    "        if len(missing_metrics) > 0:\n",
    "            for metrics_name in missing_metrics:\n",
    "                raw_metrics[metrics_name] = 0\n",
    "        assert np.sum([value for metrics_name, value in raw_metrics.items()]) == len(df)\n",
    "\n",
    "        # Save the metrics\n",
    "        inv_metrics[inv_name] = raw_metrics\n",
    "        print(f\"\\nTP: {raw_metrics['TP']}\\nFP: {raw_metrics['FP']}\\n{'-' * 10}\\n\"\\\n",
    "              f\"TN: {raw_metrics['TN']}\\nFN: {raw_metrics['FN']}\\n\")\n",
    "    metrics_types[selected_dataset] = inv_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compute the final metrics (F1-score, recall, precision, fall-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for selected_dataset in dataset_types:\n",
    "    print(\"\\n\" + \"-\" * 104, \"\\n\" + \"-\" * 50 + selected_dataset.upper() + \"-\" * 50 + \"\\n\" + \"-\" * 104)\n",
    "\n",
    "    for inv_name in inv_names:\n",
    "        print(\"\\n\" + \"-\" * 70, \"\\n\" + \"-\" * 40 + inv_name + \"-\" * 40 + \"\\n\" + \"-\" * 70)\n",
    "        metrics = metrics_types[selected_dataset][inv_name]\n",
    "\n",
    "        # Compute the metrics (i.e., recall, ...)\n",
    "        recall, miss_rate, fall_out, precision, f1_score = compute_metrics(metrics['TP'], metrics['FP'], metrics['FN'], \n",
    "                                                                           metrics['TN'], verbose = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sample]",
   "language": "python",
   "name": "conda-env-sample-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
