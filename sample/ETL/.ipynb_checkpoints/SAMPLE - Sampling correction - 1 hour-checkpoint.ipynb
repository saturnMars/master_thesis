{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "if '..' not in path:\n",
    "    path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from _library.utils import SYSTEM_NAMES_FULL, load_datasets, load_amb_cond\n",
    "from os import path,makedirs\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/vieri/projects/SAMPLE\n",
      "['Binetto 1', 'Binetto 2', 'Cantore', 'Emi', 'Soleto 1', 'Soleto 2', 'Galatina', 'Verone']\n"
     ]
    }
   ],
   "source": [
    "# Select the main folder \n",
    "%cd /mnt/data/vieri/projects/SAMPLE/\n",
    "\n",
    "# Visualize names of PV systems\n",
    "print(SYSTEM_NAMES_FULL)\n",
    "# --- 0 ---------- 1 --------- 2 ------ 3 ------ 4 --------- 5 --------- 6 -------- 7 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------- \n",
      "\t\t\t\tPV SYSTEM --> EMI \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Loading inverter data...\n",
      "EMI: OK, component data loaded (4) --> INV1, INV2, INV3, INV4\n",
      "\n",
      "Loading irradiance values...\n",
      "EMI: OK, raw irradiance data (238796 observations) have been loaded\n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "FINISHED!: All datasets have been loaded. (SYS: 4 - IRR FILE: 1)\n",
      "--------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------- \n",
      "EXAMPLE --> Emi: INV1 (FROM '2018-07-27' TO '2021-06-30': 1069 days).\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 149861 entries, 0 to 149860\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   Date/Time            149861 non-null  datetime64[ns]\n",
      " 1   Iac R (A)            149861 non-null  int64         \n",
      " 2   Iac S (A)            149861 non-null  int64         \n",
      " 3   Iac T (A)            149861 non-null  int64         \n",
      " 4   Vac R (V)            149861 non-null  int64         \n",
      " 5   Vac S (V)            149861 non-null  int64         \n",
      " 6   Vac T (V)            149861 non-null  int64         \n",
      " 7   Pac R (kW)           149861 non-null  int64         \n",
      " 8   E. totale (kWh)      149861 non-null  float64       \n",
      " 9   Cc 1 (A)             149861 non-null  int64         \n",
      " 10  Vcc 1 (V)            149861 non-null  int64         \n",
      " 11  Allarme              149861 non-null  string        \n",
      " 12  Inverter temp. (°C)  149861 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(10), string(1)\n",
      "memory usage: 14.9 MB\n"
     ]
    }
   ],
   "source": [
    "system_name = SYSTEM_NAMES_FULL[3]\n",
    "system_path, inv_data, inv_names, raw_irr_data, *_ = load_datasets(system_name, subfolder= \"Cleaned\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the irrradiance values with the main inverter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 128754 entries, 0 to 128753\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   Date/Time            128754 non-null  datetime64[ns]\n",
      " 1   Iac R (A)            128754 non-null  int64         \n",
      " 2   Iac S (A)            128754 non-null  int64         \n",
      " 3   Iac T (A)            128754 non-null  int64         \n",
      " 4   Vac R (V)            128754 non-null  int64         \n",
      " 5   Vac S (V)            128754 non-null  int64         \n",
      " 6   Vac T (V)            128754 non-null  int64         \n",
      " 7   Pac R (kW)           128754 non-null  int64         \n",
      " 8   E. totale (kWh)      128754 non-null  float64       \n",
      " 9   Cc 1 (A)             128754 non-null  int64         \n",
      " 10  Vcc 1 (V)            128754 non-null  int64         \n",
      " 11  Allarme              128754 non-null  string        \n",
      " 12  Inverter temp. (°C)  128754 non-null  int64         \n",
      " 13  Irradiance (W/mq)    128754 non-null  Int64         \n",
      "dtypes: Int64(1), datetime64[ns](1), float64(1), int64(10), string(1)\n",
      "memory usage: 14.9 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129089 entries, 0 to 129088\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   Date/Time            129089 non-null  datetime64[ns]\n",
      " 1   Iac R (A)            129089 non-null  int64         \n",
      " 2   Iac S (A)            129089 non-null  int64         \n",
      " 3   Iac T (A)            129089 non-null  int64         \n",
      " 4   Vac R (V)            129089 non-null  int64         \n",
      " 5   Vac S (V)            129089 non-null  int64         \n",
      " 6   Vac T (V)            129089 non-null  int64         \n",
      " 7   Pac R (kW)           129089 non-null  int64         \n",
      " 8   E. totale (kWh)      129089 non-null  float64       \n",
      " 9   Cc 1 (A)             129089 non-null  int64         \n",
      " 10  Vcc 1 (V)            129089 non-null  int64         \n",
      " 11  Allarme              129089 non-null  string        \n",
      " 12  Inverter temp. (°C)  129089 non-null  int64         \n",
      " 13  Irradiance (W/mq)    129089 non-null  Int64         \n",
      "dtypes: Int64(1), datetime64[ns](1), float64(1), int64(10), string(1)\n",
      "memory usage: 14.9 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 128628 entries, 0 to 128627\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   Date/Time            128628 non-null  datetime64[ns]\n",
      " 1   Iac R (A)            128628 non-null  int64         \n",
      " 2   Iac S (A)            128628 non-null  int64         \n",
      " 3   Iac T (A)            128628 non-null  int64         \n",
      " 4   Vac R (V)            128628 non-null  int64         \n",
      " 5   Vac S (V)            128628 non-null  int64         \n",
      " 6   Vac T (V)            128628 non-null  int64         \n",
      " 7   Pac R (kW)           128628 non-null  int64         \n",
      " 8   E. totale (kWh)      128628 non-null  float64       \n",
      " 9   Cc 1 (A)             128628 non-null  int64         \n",
      " 10  Vcc 1 (V)            128628 non-null  int64         \n",
      " 11  Allarme              128628 non-null  string        \n",
      " 12  Inverter temp. (°C)  128628 non-null  int64         \n",
      " 13  Irradiance (W/mq)    128628 non-null  Int64         \n",
      "dtypes: Int64(1), datetime64[ns](1), float64(1), int64(10), string(1)\n",
      "memory usage: 14.8 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 131080 entries, 0 to 131079\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   Date/Time            131080 non-null  datetime64[ns]\n",
      " 1   Iac R (A)            131080 non-null  int64         \n",
      " 2   Iac S (A)            131080 non-null  int64         \n",
      " 3   Iac T (A)            131080 non-null  int64         \n",
      " 4   Vac R (V)            131080 non-null  int64         \n",
      " 5   Vac S (V)            131080 non-null  int64         \n",
      " 6   Vac T (V)            131080 non-null  int64         \n",
      " 7   Pac R (kW)           131080 non-null  int64         \n",
      " 8   E. totale (kWh)      131080 non-null  float64       \n",
      " 9   Cc 1 (A)             131080 non-null  int64         \n",
      " 10  Vcc 1 (V)            131080 non-null  int64         \n",
      " 11  Allarme              131080 non-null  string        \n",
      " 12  Inverter temp. (°C)  131080 non-null  int64         \n",
      " 13  Irradiance (W/mq)    131080 non-null  Int64         \n",
      "dtypes: Int64(1), datetime64[ns](1), float64(1), int64(10), string(1)\n",
      "memory usage: 15.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Discard useless irradiance observations (i.e., Nan values)\n",
    "if len(raw_irr_data) > 0:\n",
    "    raw_irr_data.dropna(inplace=True)\n",
    "    raw_irr_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for inv_name in inv_names:\n",
    "        inv_data[inv_name] = inv_data[inv_name].merge(raw_irr_data, on=\"Date/Time\", how=\"inner\")\n",
    "        inv_data[inv_name].info()\n",
    "else:\n",
    "     print(\"Data not available\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# STRAT 1: Keep only the observation at the hour (e.g., 17:00)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "hourly_inv_data = dict()\n",
    "for inv_name in inv_names:\n",
    "    \n",
    "    # Clean the console output\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    print(\"\\n\",30*\"-\", inv_name, 30*\"-\")\n",
    "    df = inv_data[inv_name]\n",
    "    \n",
    "    # Isolate the minute\n",
    "    df[\"Time\"] = df[\"Date/Time\"].dt.time\n",
    "    df[\"Minute\"] = df[\"Time\"].apply(lambda time: time.minute)\n",
    "    df.drop(columns = [\"Time\"], inplace=True)\n",
    "    \n",
    "    # Keep only hourly observations\n",
    "    hourly_data = df[df[\"Minute\"] == 0]\n",
    "    hourly_data.reset_index(inplace=True, drop=True)\n",
    "    df.drop(columns = [\"Minute\"], inplace=True)\n",
    "    \n",
    "    # Detect whether there is a problem with the irradiance value --> missed \n",
    "    irr_issue = hourly_data.loc[hourly_data[\"Irradiance (W/mq)\"].isna()]\n",
    "    if len(irr_issue) != 0:\n",
    "        print(\"ISSUE!\")\n",
    "        display(irr_issue)\n",
    "\n",
    "    # Analyse potential gaps between the hours\n",
    "    hourly_data[\"Gaps [h]\"] = [delta.components[1] if not pd.isnull(delta) else 0 for delta in hourly_data[\"Date/Time\"].diff()]\n",
    "    gaps = hourly_data.groupby(by= \"Gaps [h]\").count()[\"Date/Time\"]\n",
    "    gaps = gaps.sort_values(ascending=False)\n",
    "    display(gaps.to_frame(\"Observations\"))\n",
    "    \n",
    "    # Highlight the problemataic observations \n",
    "    problematic_gaps = sorted(gaps.index.to_list())\n",
    "    min_gaps = 2\n",
    "    max_gaps = 20\n",
    "    problematic_gaps = list(filter(lambda gap: gap in range(min_gaps, max_gaps + 1) , problematic_gaps))\n",
    "    \n",
    "    if len(problematic_gaps) == 0:\n",
    "        print(\"The datasets is valid, no problematic gaps have been detected.\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"PROBLEMATIC GAPS: {problematic_gaps}\\n\")\n",
    "    \n",
    "    # For each gap (e.g., 2 hours) fix the missing observations\n",
    "    for gap in problematic_gaps: \n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # Isolate the observations with that gap \n",
    "        problematic_obs = hourly_data[hourly_data[\"Gaps [h]\"] == gap][[\"Date/Time\",\"Gaps [h]\"]]\n",
    "        \n",
    "        # Exclude the observation of midnight (00:00)\n",
    "        problematic_obs = problematic_obs[problematic_obs[\"Date/Time\"].dt.time != pd.to_datetime(\"00:00\").time()]\n",
    "        print(100*\"-\", f\"\\nGAP: {gap} hour(s)\\nProblematic observations: {len(problematic_obs)}\\n\",100*\"-\")\n",
    "        #display(problematic_obs)\n",
    "       \n",
    "        # Compute number of timestamp missed in that gap (in a gap of 3 hours --> there will be 2 timestamps missed)\n",
    "        n_timestamps_in_gap = gap - 1 \n",
    "        timestamp_missed = []\n",
    "        for delta in range(1, n_timestamps_in_gap + 1):\n",
    "            time_delta = pd.Timedelta(delta, unit=\"hour\") # Nominal behaviour --> gap: 1 hour            \n",
    "            timestamp_missed.extend((problematic_obs[\"Date/Time\"] - time_delta).tolist())\n",
    "\n",
    "        # Correct the missing timestamp\n",
    "        timestamp_missed = sorted(timestamp_missed)   \n",
    "        for idk, timestamp in enumerate(timestamp_missed):\n",
    "            \n",
    "            print(f\"TARGET ({idk +1}):\", timestamp)\n",
    "            timestamp = pd.to_datetime(timestamp)\n",
    "            \n",
    "            # Create a time window (t - 30 <= timestamp <= t + 30)\n",
    "            window_size = 55 # minutes\n",
    "            window_df = df[df[\"Date/Time\"].between(\n",
    "                    timestamp - pd.Timedelta(window_size, unit=\"minutes\"),\n",
    "                    timestamp + pd.Timedelta(window_size, unit=\"minutes\")\n",
    "                )]\n",
    "            \n",
    "            # Compute time deltas between the neighbours of the target timestamp\n",
    "            window_df[\"Delta\"] = [delta.components[2] for delta in np.abs(window_df[\"Date/Time\"] - timestamp)]\n",
    "            window_df.sort_values(by =\"Delta\", inplace=True)\n",
    "            \n",
    "            # In case the target timestamp has no neighbours --> no near observation within the hour\n",
    "            if len(window_df) == 0:\n",
    "                print(f\"--> (ZERO STRAT) No neighbours found in the time window (t - 55 min <= t <= t + 55)\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Compute the best time distance found among its neighbours\n",
    "            best_distance = window_df[\"Delta\"].tolist()[0]\n",
    "        \n",
    "            # Select the candidate according to the best distance \n",
    "            candidates = window_df[window_df[\"Delta\"] == best_distance]#.iloc[:1,:]\n",
    "            \n",
    "            # Pick the candidate\n",
    "            # STATEGY 1: In case of one candidate, pick it \n",
    "            if len(candidates) == 1:\n",
    "                final_candidate = candidates.iloc[0, :]\n",
    "                print(f\"--> (STRAT 1) Picked the observation at '{candidates.iloc[0, 0].time()}' (diff: {best_distance} mins)\\n\")\n",
    "            else: \n",
    "                # STATEGY 2: Compute averaged values among the multiple candidates \n",
    "                averaged_obs = candidates.mean(numeric_only=True)\n",
    "                print(f\"--> (STRAT 2) Computed averaged values between {len(candidates)} observations \"\\\n",
    "                      f\"({[time.strftime('%H:%M')for time in candidates['Date/Time'].dt.time.tolist()]} \"\\\n",
    "                      f\"(diff: {best_distance} mins)\\n\")\n",
    "                \n",
    "                # Round integer values (apart from the column 'E. total')\n",
    "                integer_columns = averaged_obs.index.tolist()\n",
    "                integer_columns.remove('E. totale (kWh)')\n",
    "                averaged_obs.loc[integer_columns] = averaged_obs.loc[integer_columns].round(decimals = 0)\n",
    "                \n",
    "                # Select it as the candidate\n",
    "                final_candidate = averaged_obs\n",
    "                \n",
    "            # Replace the timestamp \n",
    "            final_candidate[\"Date/Time\"] = timestamp\n",
    "            final_candidate[\"Allarme\"] = candidates.iloc[0, :][\"Allarme\"]\n",
    "            final_candidate[\"Allarme\"] = final_candidate[\"Allarme\"]\n",
    "            \n",
    "            # Attach the candidate to the main 1-hour sampled dataframe\n",
    "            hourly_data = hourly_data.append(final_candidate, ignore_index=True)\n",
    "            #display(final_candidate)\n",
    "    print(\"Finished correction of timestamps\")\n",
    "            \n",
    "    # Drop the artefact columns\n",
    "    hourly_data.drop(columns =[\"Minute\",  \"Gaps [h]\", \"Delta\"], inplace=True)\n",
    "    hourly_data.sort_values(by = [\"Date/Time\"], inplace=True)\n",
    "    hourly_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Reorder index and cast columns\n",
    "    hourly_data = hourly_data[df.columns]\n",
    "    idk_to_cast = [1, 2, 3, 4, 5, 6, 7] + [9, 10, 12, 13]\n",
    "    hourly_data.iloc[:, idk_to_cast] = hourly_data.iloc[:, idk_to_cast].astype(\"int64\")\n",
    "    \n",
    "    # Attach the computed dataframe into the dictionary of all the inverter data\n",
    "    hourly_inv_data[inv_name] = hourly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRAT 2: Compute the average value for the observation at the hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INV4] TARGET (25662/25662): 2021-06-30 22:00:00\n",
      "An averaged observation (out of 4) has been created between: 21:30 and 21:05\n",
      "\n",
      " -------------------------------------------------------------------------------- \n",
      "Finished building the new 1-hour sampled dataframe with 16465 unique timestamps\n",
      "(9197 timestamp have been skipped since missing values)\n"
     ]
    }
   ],
   "source": [
    "averaged_hourly_inv_data = dict()\n",
    "for inv_name in inv_names:\n",
    "    \n",
    "    # Clean the console output\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    print(\"\\n\",30*\"-\", inv_name, 30*\"-\")\n",
    "    df = inv_data[inv_name]\n",
    "    \n",
    "    # Isolate the minute\n",
    "    df[\"Time\"] = df[\"Date/Time\"].dt.time\n",
    "    df[\"Minute\"] = df[\"Time\"].apply(lambda time: time.minute)\n",
    "    df.drop(columns = [\"Time\"], inplace=True)\n",
    "    \n",
    "    # Keep only hourly observations\n",
    "    hours = df[df[\"Minute\"] == 0][\"Date/Time\"].tolist()\n",
    "    df.drop(columns = [\"Minute\"], inplace=True)\n",
    "    start_timestamp = hours[0]\n",
    "    end_timestamp = hours[-1] + pd.Timedelta(1, unit=\"hour\") \n",
    "    hourly_ts = pd.date_range(start_timestamp, end_timestamp, freq=\"1H\")\n",
    "    print(f\"START: {start_timestamp}\\nEND:   {end_timestamp}\\nTOTAL TIMESTAMPS: {len(hourly_ts)} observations\")\n",
    "\n",
    "    hourly_avg_df = []\n",
    "    missing_ts_counter = 0\n",
    "\n",
    "    for idk, timestamp in enumerate(hourly_ts):\n",
    "        \n",
    "        if len(hourly_ts) >= 1000:\n",
    "            clear_output(wait = True)\n",
    "            \n",
    "        print(f\"\\n[{inv_name}] TARGET ({idk+1}/{len(hourly_ts)}):\", timestamp)\n",
    "\n",
    "        # Create a time window --> [t - 55 min, t] --> e.g., [9:05:10:00]\n",
    "        window_df = df[df[\"Date/Time\"].between(timestamp - pd.Timedelta(55, unit=\"minutes\"),timestamp)]\n",
    "\n",
    "        if len(window_df) == 0:\n",
    "            missing_ts_counter += 1\n",
    "            time_window = df[df[\"Date/Time\"].between(timestamp - pd.Timedelta(12, unit=\"hours\"),timestamp)]\n",
    "\n",
    "            print(f\"WARNING: No observations available for this time.\")\n",
    "            \n",
    "            if len(time_window) > 0:\n",
    "                last_timestamp = time_window.iloc[-1, 0]\n",
    "                print(f\"         Last available: {last_timestamp.time().strftime('%H:%M')} \"\\\n",
    "                      f\"({(timestamp-last_timestamp).components[1]} hours ago)\")\n",
    "            continue\n",
    "        \n",
    "        # Compute the average values\n",
    "        averaged_observation = window_df.mean(numeric_only=True)\n",
    "        \n",
    "        # Round integer values (apart from the column 'E. total')\n",
    "        integer_columns = averaged_observation.index.tolist()\n",
    "        integer_columns.remove('E. totale (kWh)')\n",
    "        averaged_observation.loc[integer_columns] = averaged_observation.loc[integer_columns].round(decimals = 0)\n",
    "        \n",
    "        # Add the timestamp\n",
    "        averaged_observation[\"Date/Time\"] = timestamp\n",
    "        \n",
    "        # Add the 'Allarme' string\n",
    "        alarm_code = window_df.loc[window_df[\"Date/Time\"] == timestamp, \"Allarme\"].tolist()\n",
    "        if alarm_code:\n",
    "            alarm_code = alarm_code[0]\n",
    "        else:\n",
    "            alarm_code = window_df[\"Allarme\"].tolist()[0]\n",
    "        averaged_observation[\"Allarme\"] = alarm_code\n",
    "        \n",
    "        # Reorder columns\n",
    "        original_order = inv_data[inv_name].columns\n",
    "        averaged_observation = averaged_observation.reindex(index = original_order)\n",
    "\n",
    "        print(f\"An averaged observation (out of {len(window_df)}) has been created between: \"\\\n",
    "              f\"{window_df.iloc[-1,0].time().strftime('%H:%M')} and {window_df.iloc[0,0].time().strftime('%H:%M')}\")\n",
    "        #display(averaged_observation)\n",
    "        \n",
    "        # Add the averaged observation to the new dataframe\n",
    "        hourly_avg_df.append(averaged_observation)\n",
    "        \n",
    "    # Create a dictionary of data for each inverter\n",
    "    averaged_hourly_inv_data[inv_name] = pd.DataFrame(hourly_avg_df)\n",
    "    \n",
    "    # Cast to int \n",
    "    idk_to_cast = [1, 2, 3, 4, 5, 6, 7] + [9, 10, 12, 13]\n",
    "    averaged_hourly_inv_data[inv_name].iloc[:, idk_to_cast] = averaged_hourly_inv_data[inv_name].iloc[:, idk_to_cast].astype(\"int64\")\n",
    "    print(\"\\n\", \"-\"*80, f\"\\nFinished building the new 1-hour sampled dataframe with {len(hourly_avg_df)} unique timestamps\\n\" \\\n",
    "          f\"({missing_ts_counter} timestamp have been skipped since missing values)\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TO CHECK\n",
    "timestamp = pd.to_datetime(\"2016-09-01 13:00:00\")\n",
    "\n",
    "df = inv_data[inv_names[0]]\n",
    "df2 = hourly_inv_data[inv_names[0]]\n",
    "df3 = averaged_hourly_inv_data[inv_names[0]]\n",
    "\n",
    "print(50*\"-\", \"ORIGINAL\", 50* \"-\")\n",
    "df.info()\n",
    "#print(50*\"-\", \"STRAT 1\", 50* \"-\")\n",
    "df2.info()\n",
    "print(50*\"-\", \"STRAT 2\", 50* \"-\")\n",
    "df3.info()\n",
    "\n",
    "# Create a time window (t - 30 <= timestamp <= t + 30)\n",
    "window_size = 24\n",
    "window_df = df[df[\"Date/Time\"].between(\n",
    "    timestamp - pd.Timedelta(window_size, unit=\"hour\"),\n",
    "    timestamp + pd.Timedelta(window_size, unit=\"hour\")\n",
    ")]\n",
    "#display(window_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the dataset with the irradiance value and the enviromental value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag to decide whether merge the enviromental temperature retrieved from another data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ambiental conditions from a second source: FALSE\n"
     ]
    }
   ],
   "source": [
    "# Enviromental data have been found for this pv systems \n",
    "if system_name in SYSTEM_NAMES_FULL[4:7]:\n",
    "    use_amb_temp = True\n",
    "else: \n",
    "    use_amb_temp = False\n",
    "print(f\"Using ambiental conditions from a second source: {str(use_amb_temp).upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This step has been skipped. \n"
     ]
    }
   ],
   "source": [
    "if use_amb_temp:\n",
    "    \n",
    "    # Load the ambiental condition from the second data source\n",
    "    amb_cond = load_amb_cond(system_name = \"Galatina\")\n",
    "    \n",
    "    # Perfom the merge on both the datasets generated \n",
    "#    hourly_datasets = [hourly_inv_data, averaged_hourly_inv_data]\n",
    "#    names = [\"1-hour sampling\", \"1-hour averaged sampling\"]\n",
    "    \n",
    "    hourly_datasets = [averaged_hourly_inv_data]\n",
    "    names = [\"1-hour averaged sampling\"]\n",
    "    \n",
    "    for idk, dataset in enumerate(hourly_datasets):\n",
    "        print(f\"{idk +1}) [{names[idk]}] Merging enviromental data.\")\n",
    "        for inv_name in inv_names:\n",
    "            dataset[inv_name] = dataset[inv_name].merge(amb_cond, on=\"Date/Time\", how=\"inner\")\n",
    "else:\n",
    "    print(\"This step has been skipped. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV System -->  EMI\n",
      "The 1-hour averaged sampling data for 'INV1' has been saved.\n",
      "\n",
      "The 1-hour averaged sampling data for 'INV2' has been saved.\n",
      "\n",
      "The 1-hour averaged sampling data for 'INV3' has been saved.\n",
      "\n",
      "The 1-hour averaged sampling data for 'INV4' has been saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"PV System --> \", system_name.upper())\n",
    "\n",
    "# Folder names\n",
    "#saving_folder_name_strat1 = \"1-hour sampling\"\n",
    "saving_folder_name_strat2 = \"1-hour averaged sampling\"\n",
    "#saving_folder_path_strat1 = path.join(system_path, \"..\", saving_folder_name_strat1)\n",
    "saving_folder_path_strat2 = path.join(system_path, \"..\", saving_folder_name_strat2)\n",
    "\n",
    "# Create the saving folders\n",
    "#if not path.exists(saving_folder_path_strat1):\n",
    "#    makedirs(saving_folder_path_strat1)\n",
    "#    print(f\"A new saving folder has been created: {saving_folder_path_strat1}\\n\")\n",
    "if not path.exists(saving_folder_path_strat2):\n",
    "    makedirs(saving_folder_path_strat2)\n",
    "    print(f\"A new saving folder has been created: {saving_folder_path_strat2}\\n\")\n",
    "\n",
    "# Save the files as CSV files \n",
    "for inv_name in inv_names:\n",
    "#    hourly_inv_data[inv_name]\n",
    "#    # Saving the dataframes created with the strat 1 --> picked the single observation at the hour\n",
    "#    file_name_strat1 = f\"hourlySampling_{inv_name.upper()}_data.csv\"\n",
    "#    hourly_inv_data[inv_name].to_csv(path.join(saving_folder_path_strat1, file_name_strat1), index=False)\n",
    "#    print(f\"The 1-hour sampling data for '{inv_name}' has been saved.\")\n",
    "    \n",
    "    # Saving the dataframes created with the strat 2 --> Compute averaged values\n",
    "    file_name_strat2 = f\"hourlyAveragedSampling_{inv_name.upper()}_data.csv\"\n",
    "    averaged_hourly_inv_data[inv_name].to_csv(path.join(saving_folder_path_strat2, file_name_strat2), index=False)\n",
    "    print(f\"The 1-hour averaged sampling data for '{inv_name}' has been saved.\\n\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sample]",
   "language": "python",
   "name": "conda-env-sample-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
