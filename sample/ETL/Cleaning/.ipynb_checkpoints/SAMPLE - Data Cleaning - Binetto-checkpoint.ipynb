{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "if '../..' not in path:\n",
    "    path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from _library.utils import SYSTEM_NAMES_FULL, load_datasets, weighted_knn, find_outliers\n",
    "from os import path, makedirs\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/vieri/projects/SAMPLE\n",
      "['Binetto 1', 'Binetto 2', 'Cantore', 'Emi', 'Soleto 1', 'Soleto 2', 'Galatina', 'Verone']\n"
     ]
    }
   ],
   "source": [
    "# Select the main folder \n",
    "%cd /mnt/data/vieri/projects/SAMPLE/\n",
    "\n",
    "# Visualize names of PV systems\n",
    "print(SYSTEM_NAMES_FULL)\n",
    "# --- 0 ---------- 1 --------- 2 ------ 3 ------ 4 --------- 5 --------- 6 -------- 7 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the PV system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system_name = SYSTEM_NAMES[1]\n",
    "print(f\"PV SYSTEM --> {system_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "system_path, inv_data, inv_names, raw_irr_data, string_inv_data, string_inv_names = load_datasets(system_name, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Quick data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# TASK: Descriptive stats\n",
    "df = inv_data[inv_names[0]]\n",
    "cols = [\"Irradiance [Average(watts-per-meter-sq)]\", \"Voltage [Average(volts)]\"]\n",
    "for col in df.columns[1:]:\n",
    "    stats = df[col].describe()\n",
    "    stats[\"median\"] = df[col].median()\n",
    "    display(stats)\n",
    "#display(string_inv_data[string_inv_names[52]][\"Voltage [Average(volts)]\"].describe())\n",
    "\n",
    "# TASK: problematic values\n",
    "prob = df[df[col] >= 10000]\n",
    "col = \"Generated Energy [Interval Sum(kilowatt-hours)]\"\n",
    "print(f\"NEGATIVE VALUES ({round((len(prob)/len(df))*100, 2)}%):{len(prob)} --> COL: {col.split('[')[0]}\")\n",
    "#display(prob)\n",
    "\n",
    "# TASK: Check whather the columns are equal \n",
    "pair_to_check = [\n",
    "    (\"Generated Energy [Interval Sum(kilowatt-hours)]\", \"PV Energy [Interval Sum(kilowatt-hours)]\"),\n",
    "    (\"Generated Power [Average(watts)]\", \"PV Power [Average(watts)]\"),\n",
    "    (\"Generated Power [Average(watts)]\", \"DC Gen. Power [Average(watts)]\"),\n",
    "    (\"DC Gen. Power [Average(watts)]\", \"PV Power [Average(watts)]\")\n",
    "]\n",
    "for pair in pair_to_check:\n",
    "    diff = np.abs(df[pair[0]] - df[pair[1]])\n",
    "    diff = diff.dropna()\n",
    "    print(\"\\nCOMPARISON:\", \"- \".join([item.split('[')[0] for item in pair]))\n",
    "    print(\"MEAN:\", np.mean(diff))\n",
    "    print(\"STD:\", np.std(diff))\n",
    "\n",
    "# TASK: Check availability of insolation\n",
    "col = \"Insolation [Interval Sum(kilowatt-hours-per-meter-sq)]\"\n",
    "if col in df.columns:\n",
    "    prob = df[col].dropna()\n",
    "    print(f\"\\nVALUES AVAILABLE ({round((len(prob)/len(df))*100, 2)}%): {len(prob)} --> COL: {col.split('[')[0]}\")\n",
    "    #display(prob)\n",
    "\n",
    "# ANALYSE outliers of voltage\n",
    "cols = [\"Voltage AN [Average(volts)]\", \"Voltage BN [Average(volts)]\", \"Voltage CN [Average(volts)]\", \"Voltage [Average(volts)]\"]\n",
    "cond = df[cols[0]] < 210\n",
    "cond2 = df[\"Date/Time [Europe/Rome]\"].dt.time > pd.to_datetime(\"07:00\").time()\n",
    "filtered_df = df.loc[cond & cond2, [\"Date/Time [Europe/Rome]\"] + cols]\n",
    "print(f\"\\nVOLTAGE (AN/BN/CN) OUTLIERS ({round((len(filtered_df)/len(df))*100,2)}%):{len(filtered_df)}\\n\")\n",
    "#display(filtered_df.iloc[:1000,:])\n",
    "\n",
    "# Check out the cut-off point for binetto 1 (due to corrupt values of the irradiance)\n",
    "cutoff_date = pd.to_datetime(\"2020-01-14\").date()\n",
    "print(\"-\"*20, \"Checking out cutoff date (due to corrupt values of the irradiance)\", \"-\"*20)\n",
    "for pre_delta in range(4, -1, -1):\n",
    "    date = cutoff_date - pd.Timedelta(pre_delta, unit=\"day\")\n",
    "    if pre_delta == 0:\n",
    "         print(f\"DATE: {date} --> CUT-OFF DAY\")\n",
    "    else:\n",
    "        print(f\"DATE: {date} (- {pre_delta} days)\")\n",
    "    \n",
    "    values = df[df[\"Date/Time [Europe/Rome]\"].dt.date == date][\"Irradiance [Average(watts-per-meter-sq)]\"]\n",
    "    avg = np.round(np.mean(values.dropna().tolist()), 2)\n",
    "    print(f\"Irradiance (AVG): {avg} w/mq\\n\")\n",
    "for post_delta in range(1,4):\n",
    "    date = cutoff_date + pd.Timedelta(post_delta, unit=\"day\")\n",
    "    print(f\"DATE: {date} (+ {post_delta} day(s))\")\n",
    "    \n",
    "    values = df[df[\"Date/Time [Europe/Rome]\"].dt.date == date][\"Irradiance [Average(watts-per-meter-sq)]\"]\n",
    "    avg = np.round(np.mean(values.dropna().tolist()), 2)\n",
    "    print(f\"Irradiance (AVG): {avg} w/mq\\n\")\n",
    "\n",
    "# Check integer columns\n",
    "integer_columns = set()\n",
    "print(30*\"-\", \"Checking the type of the columns\", 30*\"-\")\n",
    "for k, col in enumerate(df.columns[1:]):\n",
    "    raw_values = df[col]\n",
    "    non_integer = [value for value in raw_values if not value.is_integer()]\n",
    "    non_nanInteger = [value for value in non_integer if not np.isnan(value)]\n",
    "    print(f\"Checking the column ({k+1}): {col.upper()}\")\n",
    "    if len(non_nanInteger) > 0:\n",
    "        print(f\"--> Number of non integer values (that are not NaN values): {len(non_nanInteger)} \"\\\n",
    "              f\"({round((len(non_nanInteger)/len(raw_values))*100,2)} %)\\n\")\n",
    "    else:\n",
    "        print(\"--> All integer values in this column!\\n\")\n",
    "        integer_columns.add(col)\n",
    "print(\"INTEGER COLUMNS TO CAST:\\n\")\n",
    "for idk, col in enumerate(integer_columns):\n",
    "    print(f\"{(idk + 1)}) {col}\\n\")\n",
    "\n",
    "# ------ FINDINGS --------\n",
    "# 1) DISCARD\n",
    "# ---> CELL TEMP (SYS & AMB) --> Mean -40 --> STD: 0.0\n",
    "# ---> Insolation --> data available only for the 3.5 % of the data (8K out of 228K)\n",
    "# ---> Discard PV Energy (as 'Generated Energy' = 'PV Energy')\n",
    "# ---> Discard PV Power (as 'Generated Power' = 'PV power')\n",
    "# 3) OUTLIERS TO CLEAN \n",
    "#---> (Generated Energy) --> around 0.02% negative values + other outliers (> 50000)\n",
    "# 4) OUTLIERS TO INVESTIGATE BETTER:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the two columns of ambiental temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK: Merge the two columns of the ambiental temperatures\n",
    "dt_col = \"Date/Time [Europe/Rome]\"\n",
    "\n",
    "cols = [\"Ambient Temp. [Average(celsius)]\" ,\"Ambient Temp. [Average(celsius)]_SYS\"]\n",
    "\n",
    "# Values\n",
    "amb_source = raw_irr_data[[\"Date/Time [Europe/Rome]\", \"Ambient Temp. [Average(celsius)]\"]].dropna().round(decimals = 3)\n",
    "sys_source = inv_data[inv_names[0]][[\"Date/Time [Europe/Rome]\", \"Ambient Temp. [Average(celsius)]_SYS\"]].dropna().round(decimals = 3)\n",
    "\n",
    "# Merge the two columns \n",
    "merged_temp = amb_source.merge(sys_source, how =\"outer\", on=\"Date/Time [Europe/Rome]\", indicator=True)\n",
    "display(merged_temp.groupby(by = \"_merge\").count()[[dt_col]])\n",
    "\n",
    "# Compute the difference\n",
    "diff_obs = np.abs(merged_temp[cols[1]] - merged_temp[cols[0]])\n",
    "diff_obs.dropna(inplace=True)\n",
    "mismatch_indexes = diff_obs[diff_obs > 0].index\n",
    "mismatch_timestamp = merged_temp.iloc[mismatch_indexes, 0].tolist()\n",
    "\n",
    "# Fill the missing values with the value available in the other column\n",
    "merged_temp[\"Ambiental Temp. (celsius)\"] = merged_temp[cols[0]]\n",
    "merged_temp[\"Ambiental Temp. (celsius)\"].fillna(merged_temp[cols[1]], inplace=True)\n",
    "\n",
    "# Fix the mismatces ( diff less than 1Â°C)\n",
    "mismatches = merged_temp[merged_temp[dt_col].isin(mismatch_timestamp)].index\n",
    "print(\"MISMATCHES: \", len(mismatches))\n",
    "\n",
    "# Compute an average value\n",
    "merged_temp.iloc[mismatches, -1] = merged_temp.iloc[mismatches, [1, 2]].mean(axis=1)\n",
    "\n",
    "# Discard duplicates\n",
    "merged_temp.drop_duplicates(inplace=True)\n",
    "\n",
    "# Select only the useful column\n",
    "merged_temp = merged_temp[[dt_col] + [\"Ambiental Temp. (celsius)\"]]\n",
    "\n",
    "# Identify duplicate timestamp with different temperature\n",
    "duplicated_ts = merged_temp[merged_temp['Date/Time [Europe/Rome]'].duplicated(keep=False)]\n",
    "unique_duplicated_dt = [pd.to_datetime(datetime) for datetime in duplicated_ts[\"Date/Time [Europe/Rome]\"].unique()]\n",
    "\n",
    "for timestamp in unique_duplicated_dt:\n",
    "    print(\"\\nTIMESTAMP: \", timestamp)\n",
    "    \n",
    "    # Isolate the observation\n",
    "    daily_duplicated = merged_temp[merged_temp[\"Date/Time [Europe/Rome]\"] == timestamp]\n",
    "\n",
    "    if len(daily_duplicated) == 0:\n",
    "        print(\"No observations found\")\n",
    "        continue\n",
    "    \n",
    "    # Compute the average value for these temperatures\n",
    "    averaged_value = np.round(np.mean(daily_duplicated['Ambiental Temp. (celsius)'].tolist()), 2)\n",
    "    \n",
    "    # Create a new observation\n",
    "    pair = (daily_duplicated['Date/Time [Europe/Rome]'].tolist()[0], averaged_value)\n",
    "    new_obs = pd.Series(data = pair, index = daily_duplicated.columns)\n",
    "    print(f\"--> An averaged observation has been created: {new_obs['Ambiental Temp. (celsius)']}\")\n",
    "    \n",
    "    # Add the new averaged observation in the temperature dataframe\n",
    "    merged_temp = merged_temp.append(new_obs, ignore_index=True)\n",
    "    \n",
    "    # Drop the previous observations\n",
    "    merged_temp.drop(index = daily_duplicated.index, inplace=True)\n",
    "    \n",
    "    # Sort the updated dataframe\n",
    "    merged_temp.sort_values(by = \"Date/Time [Europe/Rome]\", inplace = True)\n",
    "    merged_temp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# display(merged_temp[merged_temp[\"Date/Time [Europe/Rome]\"].dt.date >  pd.to_datetime(\"2021-05-31\").date()])\n",
    "\n",
    "# Merge the merged column with the main dataframe\n",
    "inv_data[inv_names[0]] = inv_data[inv_names[0]].merge(merged_temp, how=\"inner\", on=\"Date/Time [Europe/Rome]\")\n",
    "inv_data[inv_names[0]].drop(columns = [\"Ambient Temp. [Average(celsius)]_SYS\", \"Ambient Temp. [Average(celsius)]_SYS\"], \n",
    "                            inplace=True)\n",
    "inv_data[inv_names[0]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check uniquness of timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_datetime_uniqueness(df):\n",
    "    index_to_delate = []\n",
    "\n",
    "    # Check duplicates\n",
    "    condition = df[\"Date/Time [Europe/Rome]\"].duplicated(keep=False)\n",
    "    duplicated_datetime = df[condition]\n",
    "\n",
    "    # Continue to analyse other inverter in case of no duplicated observations\n",
    "    if len(duplicated_datetime) == 0:\n",
    "        print(f\"Oh, that's good. No duplicated observations have been found for {inv_name}!\")\n",
    "        return None\n",
    "    else:\n",
    "        # Extraxct unique datetimes\n",
    "        unique_duplicated_dt = [pd.to_datetime(datetime) for datetime in duplicated_datetime[\"Date/Time [Europe/Rome]\"].unique()]\n",
    "        print(f\"DUPLICATE DATES: {len(unique_duplicated_dt)}:\"\\\n",
    "              f\"\\n{[dt.strftime('%Y-%m-%d, %H:%M') for dt in unique_duplicated_dt]}\"\\\n",
    "              f\"\\nTotal observations: {len(duplicated_datetime)}\")\n",
    "        #display(duplicated_datetime)\n",
    "\n",
    "        # Investigate this behaviour for a duplicate observations\n",
    "        datetime_to_investigate = unique_duplicated_dt[0]\n",
    "        delta = pd.Timedelta(15, unit=\"minutes\") #datetime.timedelta(minutes=60)\n",
    "        period = (datetime_to_investigate - delta, datetime_to_investigate + delta)\n",
    "        #display(df.loc[df[\"Date/Time (Europe/Rome)\"].between(period[0], period[1]), :])\n",
    "\n",
    "        # Compute difference \n",
    "        to_discard = set()\n",
    "        \n",
    "        for datetime in unique_duplicated_dt:\n",
    "            daily_duplicated_indexes = df[df[\"Date/Time [Europe/Rome]\"] == datetime].index\n",
    "            duplicated_obs = df.loc[daily_duplicated_indexes, :].drop(columns = \"Date/Time [Europe/Rome]\")\n",
    "            print(f\"\\nAnalysing {datetime} - (observations: {len(daily_duplicated_indexes)})\")\n",
    "            print(\"-\"*120)\n",
    "\n",
    "            equal_observations = set()\n",
    "            for idk_check, index_obs in enumerate(daily_duplicated_indexes):\n",
    "                other_duplicated_obs = duplicated_obs.drop(index_obs, axis=0)\n",
    "\n",
    "                check = duplicated_obs.loc[index_obs,:].fillna(0).eq(other_duplicated_obs.fillna(0))\n",
    "                find_equal_obs = check.all(axis=1)[check.all(axis=1) == True].index.tolist()\n",
    "\n",
    "                print(f\"\\n--> Analysing idk: {index_obs}...\")                \n",
    "                if find_equal_obs:\n",
    "                    print(f\"    Equal observation(s) have been found: {find_equal_obs}\")\n",
    "                    idk_equal_obs = [index_obs] + [idk for idk in find_equal_obs]\n",
    "                    idk_equal_obs.sort()\n",
    "\n",
    "                    #display(duplicated_obs.loc[idk_equal_obs, :])\n",
    "                    idk_equal_obs = [idk for idk in find_equal_obs]\n",
    "\n",
    "                    # Save pairs of equal duplicated observations\n",
    "                    equal_observations.add(tuple(idk_equal_obs))\n",
    "                else:\n",
    "                    print(\"    No equal observation(s) have been found for the timestamp \")\n",
    "\n",
    "            # Keep the first equal and discard the other equal observations\n",
    "            duplicated_diff_obs = sorted(set(obs_to_discard for pairs in equal_observations for obs_to_discard in pairs[1:]))\n",
    "\n",
    "            if len(duplicated_diff_obs) != 0:\n",
    "                to_discard.update(duplicated_diff_obs)\n",
    "                print(F\"\\n--> OK, discarding the identical observations ({len(duplicated_diff_obs)}/{len(daily_duplicated_indexes)}): {duplicated_diff_obs}\")\n",
    "                \n",
    "            # --------------------------------------- STRAT 2 --------------------------------------------\n",
    "            \n",
    "            # Highlight potential issues: same timestamp not equal values\n",
    "            remaining_duplicated_obs = sorted(set(daily_duplicated_indexes) - set(duplicated_diff_obs))\n",
    "\n",
    "            if len(remaining_duplicated_obs) > 1:\n",
    "                print(f\"\\n\\n--> Issue, duplicated observations with different values have been found! --> {remaining_duplicated_obs}\")\n",
    "                remaining_df = df.loc[remaining_duplicated_obs, :]\n",
    "                unique_dt = [pd.to_datetime(datetime) for datetime in remaining_df[\"Date/Time [Europe/Rome]\"].unique()]\n",
    "                \n",
    "                obs_to_discart = []\n",
    "                for datetime in unique_dt:\n",
    "                    daily_duplicated_indexes = df[df[\"Date/Time [Europe/Rome]\"] == datetime].index\n",
    "                    obs_to_discard = set(daily_duplicated_indexes.tolist()[1:]) - to_discard\n",
    "                    print(\"    STRAT 2 (TO DISCARD):\", obs_to_discard)\n",
    "                    to_discard.update(obs_to_discard)\n",
    "    \n",
    "        # Select the index to remove (keep the first one, as they are the equal)\n",
    "        #idx_to_delate = set(obs_to_discard for pairs in equal_observations for obs_to_discard in pairs[1:])\n",
    "        #item_to_delate = sorted(idx_to_delate.union(to_discard))\n",
    "        item_to_delate = sorted(to_discard)\n",
    "        print(\"\\n\", *\"-\")\n",
    "        print(f\"TO DELATE ({len(item_to_delate)} out of {len(duplicated_datetime)}):\")\n",
    "        print(item_to_delate, \"\\n\", 80*\"-\")\n",
    "\n",
    "        return item_to_delate\n",
    "#-----------------------------------\n",
    "item_to_delate = check_datetime_uniqueness(inv_data[inv_names[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARRY OUT SOME TRANSFORMATION\n",
    "1. Discard duplicated observations (i.e., time changes: 2 AM - 3 AM)\n",
    "2. Discard *unnecessary columns*\n",
    "3. [only for Binetto 1] Discard observations before a cut-off date due to currupted irradiance values. \n",
    "4. Cast columns (from *'float64'* to *'int64'*)\n",
    "5. *Value transformation* (from 'watt' to 'kilowatt')\n",
    "6. Rename columns to remove the unnecessary text (i.e., \"Average\")\n",
    "7. Reorder columns *improve readability*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TASK A: Drop duplicated indexes\n",
    "print(f\"MAIN INVERTER: {system_name.upper()}:System\")\n",
    "inv_data[inv_names[0]].drop(index = item_to_delate, inplace=True)\n",
    "inv_data[inv_names[0]].reset_index(inplace = True, drop=True)\n",
    "print(f\"A) Some ({len(item_to_delate)}) duplicated rows (i.e., due to the time changes) have been DISCARTED\\n\")\n",
    "\n",
    "# TASK B: DISCARD COLUMN\n",
    "columns_to_discard = [\n",
    "    \"Cell Temp. [Average(celsius)]_SYS\", \n",
    "    \"Cell Temp. [Average(celsius)]_AMB\", \n",
    "    \"PV Energy [Interval Sum(kilowatt-hours)]\",\n",
    "    \"PV Power [Average(watts)]\"\n",
    "]\n",
    "if system_name == SYSTEM_NAMES[0]:\n",
    "    columns_to_discard.append(\"Insolation [Interval Sum(kilowatt-hours-per-meter-sq)]\")   \n",
    "inv_data[inv_names[0]].drop(columns = columns_to_discard, inplace=True)\n",
    "print(f\"B) Some columns ({len(columns_to_discard)}) have been DISCARTED. \\n   {[item.split('[')[0].rstrip() for item in columns_to_discard]}\\n\")\n",
    "\n",
    "# TASC C: Keep only observation after the cutoff date (before the corrupted irradiance values)\n",
    "if system_name == SYSTEM_NAMES[0]:\n",
    "    cond = inv_data[inv_names[0]][\"Date/Time [Europe/Rome]\"].dt.date > cutoff_date\n",
    "    filtered_df = inv_data[inv_names[0]][cond]\n",
    "    ratio_discarted = round(((len(inv_data[inv_names[0]]) - len(filtered_df)) / len(inv_data[inv_names[0]])) *100,2)\n",
    "    inv_data[inv_names[0]] = filtered_df\n",
    "    print(f\"C) [only for Binetto 1] A period covered by the dataset ({ratio_discarted} %) has been DISCARTED due to currupted irradiance values. \\n   \"\\\n",
    "          f\"(observations after {cutoff_date} have been kept)\\n\")\n",
    "    inv_data[inv_names[0]].reset_index(inplace=True, drop=True)\n",
    "    \n",
    "# TASK E: Value transformation (from Watt to kilowatt) --> for improving the coherence and readability\n",
    "watt_cols = [\"DC Gen. Power [Average(watts)]\", \"Generated Power [Average(watts)]\"]\n",
    "to_kilowatt = lambda watt_value: watt_value/1000\n",
    "for w_col in watt_cols: \n",
    "    inv_data[inv_names[0]][w_col] = to_kilowatt(inv_data[inv_names[0]][w_col])\n",
    "    inv_data[inv_names[0]].rename(columns = {w_col: w_col.replace(\"watt\", \"kilowatt\")}, inplace=True)\n",
    "print(f\"D) The watts ({len(watt_cols)} columns) have been TRANSFORMED into kilowatts\\n   \"\\\n",
    "      f\"{[item.split('[')[0].rstrip() for item in watt_cols]}\\n\")\n",
    "\n",
    "# TASK D: Cast variables (i.e., from 'float64' to 'int64')\n",
    "columns_to_cast = integer_columns - set(columns_to_discard) - set(watt_cols)\n",
    "if len(columns_to_cast) > 0:\n",
    "    for int_col in columns_to_cast:\n",
    "        inv_data[inv_names[0]][int_col] = inv_data[inv_names[0]][int_col].astype(\"Int64\")\n",
    "    print(f\"E) Some columns ({len(columns_to_cast)}) have been CAST (from 'float64' to 'int64').\\n   \"\\\n",
    "          f\"{[item.split('[')[0].rstrip() for item in columns_to_cast]}\\n\")\n",
    "\n",
    "# TASK F: RENAME COLUMNS \n",
    "col_names = inv_data[inv_names[0]].columns\n",
    "col_names = [col.replace(\"[Average\", \"\").replace(\"[Interval Sum\", \"\").rstrip(']') for col in col_names]\n",
    "col_names[0] = \"Date/Time (Europe/Rome)\"\n",
    "inv_data[inv_names[0]].columns = col_names \n",
    "print(\"F) All the columns have been RENAMED to improve the readability\\n\")\n",
    "\n",
    "# TASK G: REORDER COLUMNS\n",
    "new_col_order = [\n",
    "    \"Date/Time (Europe/Rome)\",\n",
    "    \"Voltage AN (volts)\",\n",
    "    \"Voltage BN (volts)\",\n",
    "    \"Voltage CN (volts)\",\n",
    "    \"Voltage (volts)\",\n",
    "    \"Current (amps)\",\n",
    "    \"Generated Power (kilowatts)\",\n",
    "    \"Generated Energy (kilowatt-hours)\",\n",
    "    \"DC Voltage (volts)\",\n",
    "    \"DC Gen. Power (kilowatts)\",\n",
    "    \"Ambiental Temp. (celsius)\",\n",
    "    \"Irradiance (watts-per-meter-sq)\"\n",
    "]\n",
    "inv_data[inv_names[0]] = inv_data[inv_names[0]].reindex(columns = new_col_order)\n",
    "print(\"G) The columns have been SORTED to improve the readability.\\n\")\n",
    "\n",
    "# VISUALIZE OUTCOMES\n",
    "print(\"-\"*40, \"THE OUTCOME\",\"-\"*40)\n",
    "inv_data[inv_names[0]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARIABLE: *Generated Energy (kWh)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def outlier_correction_gen_energy(df): #, kWp, tolerance_max_power = 0.3, threshold_zscore = 3\n",
    "    columns = [\"Date/Time (Europe/Rome)\", \"Generated Energy (kilowatt-hours)\"]\n",
    "\n",
    "    # Use different strategies to find the abnormal values \n",
    "    # STRAT 1: Find negative values\n",
    "    neg_cond = df[\"Generated Energy (kilowatt-hours)\"] < 0\n",
    "    negative_values = df[neg_cond]\n",
    "    print(\"-\"*20, f\"NEGATIVE kWh VALUES ({len(negative_values)})\", \"-\"*20)\n",
    "    \n",
    "    # STRAT 2: Find values beyond the theoretical limit\n",
    "    # Compute the theoretical maximum\n",
    "    #h = 1\n",
    "    #tolerance = 1 + tolerance_max_power # Add a tolerance of 30 %\n",
    "    #max_theoretical_kwh = (kWp * h) * tolerance\n",
    "    #print(f\"THEORETICAL MAX: {max_theoretical_kwh} kWh (kWp system: {kWp})\")\n",
    "    \n",
    "    #theoretical_invalid_values = df[df[\"Generated Energy (kilowatt-hours)\"] > max_theoretical_kwh]\n",
    "    #print(\"-\"*10, f\"kWh VALUES ({len(theoretical_invalid_values)}) BEYOND THE THEORETICAL LIMIT (kWp * h)\", \"-\"*10)\n",
    "    #if len(negative_values) > 0:\n",
    "        #display(theoretical_invalid_values.iloc[:, [0, 4, 5, 6, 7, -1]])\n",
    "    #else:\n",
    "        #print(\"No values beyond the theoretical limit have been found. That's good. \\n\")\n",
    "\n",
    "    # STRAT 3: Find outliers by using the z-score\n",
    "    #extreme_outlier = find_outliers(df[columns], verbose=True, threshold = threshold_zscore)\n",
    "\n",
    "    # Merge all the outliers discovered with these three strategies\n",
    "    #idk_outliers = sorted(set(negative_values.index.tolist() + theoretical_invalid_values.index.tolist() + extreme_outlier.index.tolist()))\n",
    "    idk_outliers = negative_values.index.tolist()# + high_values.index.tolist()\n",
    "    \n",
    "    if len(idk_outliers) == 0:\n",
    "        print(\"Oh, no extreme outliers found. That's good.\\n\")\n",
    "    else:\n",
    "        display(negative_values)\n",
    "        #print(50 * \"-\", f\"\\n{len(idk_outliers)} outliers have been found for the variable 'Generated Energy'. \\n\")\n",
    "\n",
    "        # Set a copy of the outlier indexes used to remove them after correcting them\n",
    "        list_outliers = idk_outliers.copy()\n",
    "\n",
    "        # Compute and assign the estimated value (weighted average value from its neigbours)\n",
    "        number_neighbours = 6 \n",
    "        for idk, idk_outlier in enumerate(idk_outliers):\n",
    "\n",
    "            # Visualize the outlier and its neighours\n",
    "            print(f\"\\nOutlier {idk + 1}/{len(idk_outliers)} (idk: {idk_outlier}) \"\\\n",
    "                  f\"and its neighborhood [{number_neighbours//2} || {number_neighbours//2}]\")\n",
    "            display(df[columns].loc[range(idk_outlier - number_neighbours//2, idk_outlier + number_neighbours//2 + 1), :])\n",
    "\n",
    "            # Correct the outlier \n",
    "            computed_value = weighted_knn(df[columns], idk_outlier, list_outliers, \"Generated Energy (kilowatt-hours)\",\n",
    "                                          K = number_neighbours, verbose=True)\n",
    "            df.loc[idk_outlier, \"Generated Energy (kilowatt-hours)\"] = computed_value\n",
    "    print(20*\"-\",\"FINISHED\",20*\"-\")\n",
    "    return df\n",
    "# ----------------------------------------\n",
    "#nominal_max_power = 997.92 # SOURCE: Binetto X - Schema unifilare\n",
    "inv_data[inv_names[0]] = outlier_correction_gen_energy(inv_data[inv_names[0]])\n",
    "\n",
    "# FINDINGS\n",
    "# STRAT 1: It's perfectly legitime (negative values are not valid)\n",
    "# STRAT 2: Theoretical limit --> the variable is an \"interval sum\" of the energy generated.\n",
    "# ---> Possible issue --> it's up to the sampling? kWh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARIABLE: **Voltage  & Voltage(AN/BN/CN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Set the dataset\n",
    "df = inv_data[inv_names[0]]\n",
    "columns = [\"Date/Time (Europe/Rome)\", \"Voltage (volts)\", \"Voltage AN (volts)\", \"Voltage BN (volts)\", \"Voltage CN (volts)\"]\n",
    "\n",
    "# 1A) Detect extreme outliers \n",
    "extreme_vac_outlier = find_outliers(df[columns], threshold = 5, verbose=True,)\n",
    "\n",
    "if len(extreme_vac_outlier) == 0:\n",
    "    print(\"Oh, no extreme outliers found. That's good.\\n\")\n",
    "else:\n",
    "    #display(extreme_vac_outlier)\n",
    "    \n",
    "    # Carry out a first analysis\n",
    "    # a) Hours of the outliers\n",
    "    extreme_vac_outlier[\"Hour\"] = extreme_vac_outlier[\"Date/Time (Europe/Rome)\"].dt.time.apply(lambda time: time.strftime(\"%H\"))\n",
    "    \n",
    "    # b) Group according to zero and non-zero values\n",
    "    extreme_vac_outlier[\"Zero Vac values\"] = extreme_vac_outlier[\"Voltage (volts)\"].apply(lambda voltage: \"Yes\" if voltage == 0 else \"No\")\n",
    "    grouped_vac_outlier = extreme_vac_outlier.groupby(by =[\"Hour\", \"Zero Vac values\"]).count()[\"Date/Time (Europe/Rome)\"]\n",
    "    display(grouped_vac_outlier.to_frame())\n",
    "    \n",
    "    zero_values = extreme_vac_outlier[extreme_vac_outlier[\"Voltage (volts)\"] == 0]\n",
    "    print(f\"Extreme outliers ({len(extreme_vac_outlier)}) of Vac have found \"\\\n",
    "          f\"(zero values: {round((len(zero_values)/len(extreme_vac_outlier))*100)} %)\")\n",
    "    \n",
    "    # Identfy the index of the outliers \n",
    "    idk_outliers = extreme_vac_outlier.index.tolist()\n",
    "        \n",
    "# ---- FINDINGS -----\n",
    "# The outliers are many (i.e., around 1K) and they are scattered throughout the day \n",
    "# (even though the most of the outiliers are in the morning hours).\n",
    "# The zero values are around 33 % of these outliers. \n",
    "# Therefore, considering this behaviour and the importance of these variables (AC Voltages) for the future ML model\n",
    "\n",
    "# STARTEGY --> Do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARIABLE: **Irradiance (watts-per-meter-sq)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = inv_data[inv_names[0]]\n",
    "irr = df[\"Irradiance (watts-per-meter-sq)\"]\n",
    "\n",
    "# Count zero values\n",
    "irr_zero_values = irr[irr == 0]\n",
    "available_pos_values = irr[irr != 0].dropna()\n",
    "print(f\"\\nZERO VALUES: {len(irr_zero_values)} ({ round((len(irr_zero_values)/len(irr))*100, 2) } %)\")\n",
    "print(f\"AVAILABLE POSITIVE VALUES: {len(available_pos_values)} ({ round((len(available_pos_values)/len(irr))*100, 2) } %)\\n\")\n",
    "print(\"-\" * 20, \"DESCRIPTIVE STATISTICS\", \"-\" * 20)\n",
    "display(irr.describe())\n",
    "\n",
    "# Investigate high values\n",
    "emp_threshold = 1300\n",
    "likely_outliers = df[irr > emp_threshold]\n",
    "print(\"\\n\",\"-\" * 20, f\"PROBABLE OUTLIERS (> {emp_threshold} w/mq)\", \"-\" * 20)\n",
    "display(likely_outliers)\n",
    "\n",
    "# Visualize the neighbours of a possible outlier \n",
    "idk = likely_outliers.index.tolist()[0]\n",
    "window = 2\n",
    "print(\"-\" * 20, \"VISUALIZE AN EXAMPLE OF OUTLIERS\" , \"-\" * 20 )\n",
    "print(f\"IDK: {idk} ({df.iloc[idk, 0]}) and its neighbourhood ({window} | idk | {window})\")\n",
    "display(df.iloc[idk - window:idk + window + 1,:])\n",
    "\n",
    "# ---- FINDINGS -----\n",
    "# The values beyond an emperical threshold of 1400 w/mq seems to be valid\n",
    "# The neighbours are often different, but other parameters (Current, Power) seem to be coherent with this high irradiance value\n",
    "\n",
    "# STARTEGY --> Do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRING INVERTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --------------- STRING INVERTER: Special -----------------------------\n",
    "idk_special = 52 if system_name == SYSTEM_NAMES[0] else 0\n",
    "special_df = string_inv_data[string_inv_names[idk_special]]\n",
    "\n",
    "# Visualize info columns\n",
    "print(30 * \"-\", f\"Special STRING INVERTER ({string_inv_names[idk_special]})\", 30 * \"-\", \"\\n\")\n",
    "special_df.info()\n",
    "\n",
    "# Check out the difference between the variables (Generated Energy and PV Energy)\n",
    "cols = [\"Generated Energy [Interval Sum(kilowatt-hours)]\", \"PV Energy [Interval Sum(kilowatt-hours)]\"] \n",
    "energy = special_df[[\"Date/Time [Europe/Rome]\"] + cols]\n",
    "energy = energy.dropna(subset = cols)\n",
    "diff = np.abs(energy[cols[0]] - energy[cols[1]])\n",
    "print(\"\\n\", 20 * \"-\", f\"COMPARISON between '{cols[0].split('[')[0].rstrip().upper()}' and '{cols[1].split('[')[0].rstrip().upper()}'\", 20 * \"-\",)\n",
    "print(f\"MEAN (abs diff): {np.mean(diff)}\\nSTD (abs diff): {np.std(diff)}\")\n",
    "\n",
    "# Check out the column of DC power\n",
    "dc_power_col = \"DC Gen. Power [Average(watts)]\"\n",
    "if dc_power_col in special_df.columns:\n",
    "    dc_power = special_df[~ special_df[dc_power_col].isnull()]\n",
    "    print(20 * \"-\", f\"Checking out the variable {dc_power_col.split('[')[0]}\", 20 * \"-\")\n",
    "    print(f\"'{dc_power_col.split('[')[0].rstrip()}' column is available only for the {round((len(dc_power) / len(special_df))*100, 2)} % \"\\\n",
    "          f\"of the entire dataset ({int(round(len(special_df)/1000, 0))} K obs.)\")\n",
    "    print(f\"PERIOD AVAILABLE: FROM '{dc_power.iloc[0,0].date()}' TO '{dc_power.iloc[-1,0].date()}' ({(dc_power.iloc[-1,0] - dc_power.iloc[0,0]).components[0]} days)\\n\", 90 * \"-\")\n",
    "\n",
    "# ------ FINDINGS ------\n",
    "# 1) CHECK OUT --> Generated energy = PV energy\n",
    "# 2) OUTLIERS\n",
    "# ---> Generated Energy \n",
    "# ---> DC VOLTAGE --> 0 values\n",
    "# ---> Voltage (+ AN/BN/CN) --> 0 Values \n",
    "\n",
    "# --------------- STRING INVERTER: Standard -----------------------------\n",
    "idk = 0\n",
    "example_df = string_inv_data[string_inv_names[idk]]\n",
    "print(\"\\n\\n\", 40 * \"-\", \"Standard STRING INVERTER\", 40 * \"-\", \"\\n\")\n",
    "example_df.info()\n",
    "\n",
    "# Count PV POWER observation\n",
    "col = \"PV Power [Average(watts)]\"\n",
    "if col in example_df.columns:\n",
    "    pv_power = example_df[~ example_df[col].isnull()]\n",
    "    print(\"\\n\", 20 * \"-\", f\"Checking out the variable {col.split('[')[0]}\", 20 * \"-\")\n",
    "    print(f\"'{col.split('[')[0].rstrip()}' column is available only for the {round((len(pv_power) / len(example_df))*100, 2)} % \"\\\n",
    "          f\"of the entire dataset ({int(round(len(example_df)/1000, 0))} K obs.)\")\n",
    "    print(f\"PERIOD:FROM '{pv_power.iloc[0,0].date()}' TO '{pv_power.iloc[-1,0].date()}'\"\\\n",
    "          f\"({(pv_power.iloc[-1,0] - pv_power.iloc[0,0]).components[0]} days)\\n\", 90 * \"-\")\n",
    "\n",
    "for col in example_df.columns[1:]:\n",
    "    stats = example_df[col].describe()\n",
    "    stats[\"median\"] = example_df[col].median()\n",
    "    display(stats)\n",
    "    \n",
    "# ------ FINDINGS ------\n",
    "# 1) DISCARD \n",
    "# --> PV Power --> it's available only for the 8% of the entire dataset ()\n",
    "# --> [Special inverter] DC Gen. Power: it's available only for the 5% of the entire dataset (2021/05 - 2021/06)\n",
    "# --> [Special inverter] PV Energy --> it's a duplicated column (= Generated Energy)\n",
    "# 2) OUTLIERS\n",
    "# --> Generated energy (negative + out of range: > 500, maybe 100)\n",
    "# --> DC voltage --> 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_to_delate = dict()\n",
    "for inv_str_name in string_inv_names:\n",
    "    item_to_delate[inv_str_name] = check_datetime_uniqueness(string_inv_data[inv_str_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carry out some transformation \n",
    "1. **Discarted** duplicated observations (due to the time changes).\n",
    "2. **Discard** some (3) *columns*: \n",
    "    1. 'PV Power' due to its limited availbility *(8%: 2021/05 - 2021/07)* of the entire dataset.\n",
    "    2. *[Special String Inverter]* 'DC Gen. Power' due to its limited availbility *(5%: 2021/05 - 2021/06)* of the entire dataset..\n",
    "    3. *[Special String Inverter]* 'PV Energy' is a duplicated column. (= Generated Energy)\n",
    "2. **Rename** the *columns* to improve the readiability and coherence\n",
    "3. **Convert** the *watts (W)* in *kilowatts (kW)* for the variable 'Generated Power' to improve the coherence\n",
    "4. **Merge** the string inverter data with the *ambiental conditions (Temp. & Irradiance)*.\n",
    "5. **Reorder** the *columns* to improve the readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset of ambiental condition ambiental condition (i.e., Temperature & Irradiance)\n",
    "# amb_cond_df = inv_data[inv_names[0]][[\"Date/Time (Europe/Rome)\", \"Irradiance (watts-per-meter-sq)\", \"Ambiental Temp. (celsius)\"]]\n",
    "irr_values = raw_irr_data[[\"Date/Time [Europe/Rome]\", \"Irradiance [Average(watts-per-meter-sq)]\"]]\n",
    "if system_name == SYSTEM_NAMES[0]:\n",
    "    cond = irr_values[\"Date/Time [Europe/Rome]\"].dt.date > cutoff_date\n",
    "    irr_values = irr_values[cond]\n",
    "temp_values = merged_temp\n",
    "amb_cond_df = irr_values.merge(temp_values, how=\"inner\")\n",
    "amb_cond_df.rename(\n",
    "    columns = {\n",
    "        \"Date/Time [Europe/Rome]\": \"Date/Time (Europe/Rome)\",\n",
    "        \"Irradiance [Average(watts-per-meter-sq)]\": \"Irradiance (watts-per-meter-sq)\"},\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "# TASK: Carry out some transformation for each string inverter\n",
    "for name in string_inv_names:\n",
    "    print(\"\\n\", 30*\"-\", \"STRING INVERTER:\", name,  30*\"-\", \"\\n\")\n",
    "    \n",
    "    # TASK A: Drop duplicated indexes\n",
    "    string_inv_data[name].drop(index = item_to_delate[name], inplace=True)\n",
    "    string_inv_data[name].reset_index(inplace = True, drop=True)\n",
    "    print(f\"A) Some ({len(item_to_delate[name])}) duplicated rows (i.e., due to the time changes) have been DISCARTED\")\n",
    "    \n",
    "    # TASK A.2: Drop all empty observations for the special inverter (timestamps always available, but data starts in 2021)\n",
    "    if (system_name == SYSTEM_NAMES[0] and name == \"INV53\"):\n",
    "        string_inv_data[name].dropna(subset = string_inv_data[name].columns[1:], how=\"all\", inplace=True)\n",
    "        string_inv_data[name].reset_index(inplace = True, drop=True)\n",
    "        display(string_inv_data[name])\n",
    "        \n",
    "    # TASK B: Discard some columns\n",
    "    col_to_discard = []\n",
    "    if system_name == SYSTEM_NAMES[0]:\n",
    "        col_to_discard.append(\"PV Power [Average(watts)]\")\n",
    "        if name == \"INV53\":\n",
    "            col_to_discard.append(\"PV Energy [Interval Sum(kilowatt-hours)]\")\n",
    "            col_to_discard.append(\"DC Gen. Power [Average(watts)]\")\n",
    "    else:\n",
    "        if name == \"INV01\":\n",
    "            col_to_discard.append(\"PV Energy [Interval Sum(kilowatt-hours)]\")\n",
    "    if len(col_to_discard) > 0:\n",
    "        string_inv_data[name].drop(columns = col_to_discard, inplace=True)\n",
    "        print(f\"\\nB) {len(col_to_discard)} columns has been discarted (due to their limitated availability or because their were duplicated).\\n   \" \\\n",
    "              f\"{[item.split('[')[0].rstrip() for item in col_to_discard]}\")\n",
    "    else:\n",
    "        print(f\"\\nB) All the columns have been selected. There's no need to discard any columns\")\n",
    "    \n",
    "    # TASK C: Rename columns \n",
    "    # 1) Remove unnecessary words\n",
    "    col_names = string_inv_data[name].columns\n",
    "    col_names = [col.replace(\"[Average\", \"\").replace(\"[Interval Sum\", \"\").rstrip(']') for col in col_names]\n",
    "    \n",
    "    # 2) Use round brackets instead of square brackets (to improve visual coherence) \n",
    "    col_names[0] =  \"Date/Time (Europe/Rome)\"\n",
    "    \n",
    "    # 3) Rename the two DC current columns\n",
    "    dc_current_idk_cols  = [col_names.index(col) for col in col_names if col.startswith('DC Current')]\n",
    "    suffixes = [\"A\", \"B\"]\n",
    "    for idk, col_idk in enumerate(dc_current_idk_cols):\n",
    "        col_parts = col_names[col_idk].split(\"(\")\n",
    "        col_name = col_parts[0]\n",
    "        measure_unit = \" (\" + col_parts[1].split(\")\")[0] + \")\"\n",
    "        col_names[col_idk] = col_name + suffixes[idk] + measure_unit \n",
    "\n",
    "    # 4) Set the new column names to the dataframe\n",
    "    string_inv_data[name].columns = col_names \n",
    "    print(\"\\nC) All the columns have been RENAMED to improve the readability.\")\n",
    "    \n",
    "    # TASK D: to watt to kilowatt\n",
    "    watt_cols = [col for col in col_names if \"watts\" in col] #[\"Generated Power (watts)\"]\n",
    "    to_kilowatt = lambda watt_value: watt_value/1000\n",
    "    for col in watt_cols:\n",
    "        string_inv_data[name][col] = to_kilowatt(string_inv_data[name][col])\n",
    "        string_inv_data[name].rename(columns = {col : col.replace(\"watt\", \"kilowatt\")}, inplace=True)\n",
    "    print(f\"\\nD) The watts(W) values included in {len(watt_cols)} column(s) have been TRANSFORMED into kilowatts(kW).\\n   \"\\\n",
    "          f\"{[item.split('[')[0].rstrip() for item in watt_cols]}\")\n",
    "\n",
    "    # TASK E: Merge the string inverter data with the ambiental condition\n",
    "    string_inv_data[name] = string_inv_data[name].merge(amb_cond_df, how=\"inner\")\n",
    "    print(f\"\\nE) The string inverter data ({name}) has been merged with the ambiental conditions (Amb. Temp & irradiance).\")\n",
    "\n",
    "    # TASK F: Reorder columns \n",
    "    # Columns of the special string inverter\n",
    "    if (system_name == SYSTEM_NAMES[0] and name == \"INV53\") or (system_name == SYSTEM_NAMES[1] and name == \"INV01\"):\n",
    "        new_col_order = [\n",
    "            \"Date/Time (Europe/Rome)\",\n",
    "            \"Voltage AN (volts)\",\n",
    "            \"Voltage BN (volts)\",\n",
    "            \"Voltage CN (volts)\",\n",
    "            \"Voltage (volts)\",\n",
    "            \"Current (amps)\",\n",
    "            \"Generated Power (kilowatts)\",\n",
    "            \"Generated Energy (kilowatt-hours)\",\n",
    "            \"DC Voltage (volts)\",\n",
    "            \"DC Current A (amps)\",\n",
    "            \"DC Current B (amps)\",\n",
    "            \"Device Temp. (celsius)\",\n",
    "            \"Ambiental Temp. (celsius)\",\n",
    "            \"Irradiance (watts-per-meter-sq)\"\n",
    "        ]\n",
    "    else:\n",
    "         # Columns of the standard string inverter\n",
    "        new_col_order =  [\n",
    "            \"Date/Time (Europe/Rome)\",\n",
    "            \"DC Voltage (volts)\", \n",
    "            \"DC Current A (amps)\",\n",
    "            \"DC Current B (amps)\",\n",
    "            \"Generated Power (kilowatts)\", \n",
    "            \"Generated Energy (kilowatt-hours)\",\n",
    "            \"Ambiental Temp. (celsius)\",\n",
    "            \"Irradiance (watts-per-meter-sq)\"\n",
    "        ] \n",
    "    string_inv_data[name] = string_inv_data[name].reindex(columns = new_col_order)\n",
    "    print(f\"\\nF) The ({len(string_inv_data[name].columns)}) columns have been SORTED to improve the readability.\\n\")\n",
    "    \n",
    "    # Final version of the dataframe\n",
    "    print(40 * \"-\", \"OUTCOME\", 40 * \"-\")\n",
    "    string_inv_data[name].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLIERS CORRECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIABLE: *Generated Energy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for str_inv_name in string_inv_names:\n",
    "    print(\"\\n\", 50*\"-\", f\"STRING INVERTER: {str_inv_name}\", 50*\"-\")\n",
    "    \n",
    "    # Currect negative energy values \n",
    "    string_inv_data[name] = outlier_correction_gen_energy(string_inv_data[str_inv_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIABLE: *DC Voltage*\n",
    "ISSUE: Zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for str_inv_name in string_inv_names:\n",
    "    print(50*\"-\", f\"STRING INVERTER: {str_inv_name}\", 50*\"-\")\n",
    "    \n",
    "    # Find the observation with the variable equal to zero (i.e., the problematic behaviour)\n",
    "    zero_values = string_inv_data[str_inv_name][string_inv_data[str_inv_name][\"DC Voltage (volts)\"] == 0].index.tolist()\n",
    "    print(f\"Occurrences with the DC voltage equal to zero: {len(zero_values)}\")\n",
    "    \n",
    "    for idk_obs in zero_values:\n",
    "        to_visualize = string_inv_data[str_inv_name].iloc[idk_obs - 1: idk_obs + 2, :]\n",
    "        print(\"PREVIOUS VALUE: \",to_visualize.iloc[0, 1])\n",
    "        #print(\"OUTLIER:        \", string_inv_data[str_inv_name].iloc[idk_obs, 1])\n",
    "        print(\"NEXT VALUE:     \", round(to_visualize.iloc[-1, 1], 2) ,\"\\n\")\n",
    "        \n",
    "\n",
    "# ----- FINDINGS -------\n",
    "# The occurrences of zero values of the variable DC Voltage occurr when there's the start-up phase \n",
    "# BEFORE: NaN values || OBS: DC Voltage = 0 || AFTER: Positive values \n",
    "# ---> STARTEGY ---> Do nothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIABLE: *AC Voltages (AN/BN/CN & avg)* \n",
    "####  [ONLY SPECIAL STRING INV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [\"Date/Time (Europe/Rome)\", \"Voltage AN (volts)\", \"Voltage BN (volts)\", \"Voltage CN (volts)\", \"Voltage (volts)\"]\n",
    "\n",
    "# Set the dataset\n",
    "df = string_inv_data[string_inv_names[idk_special]]\n",
    "\n",
    "# 1A) Detect extreme outliers \n",
    "extreme_vac_outlier = find_outliers(df[columns], verbose=True, threshold = 5)\n",
    "\n",
    "if len(extreme_vac_outlier) == 0:\n",
    "    print(\"Oh, no extreme outliers found. That's good.\\n\")\n",
    "else:\n",
    "    print(f\"The extreme outliers ({len(extreme_vac_outlier)}) of Vac.\")\n",
    "    display(extreme_vac_outlier)\n",
    "    \n",
    "    # Identfy the index of the outliers \n",
    "    idk_outliers = extreme_vac_outlier.index.tolist()\n",
    "    list_outliers = idk_outliers.copy()\n",
    "\n",
    "    # Compute and assign the estimated value (weighted average value from its neigbours)\n",
    "    for idk_outlier in idk_outliers:\n",
    "\n",
    "        # Visualize the outlier and its neighours\n",
    "        print(f\"Outlier (idk: {idk_outlier}) and its neighborhood\")\n",
    "        display(df.loc[range(idk_outlier - 3, idk_outlier + 4), :])\n",
    "\n",
    "        # Compute the estimated value and assign the the original dataframe\n",
    "        df.loc[idk_outlier, \"Voltage (volts)\"] = weighted_knn(df[columns], idk_outlier, list_outliers, \"Voltage (volts)\",Fill_nan = False)\n",
    "        df.loc[idk_outlier, \"Voltage AN (volts)\"] = weighted_knn(df[columns], idk_outlier, list_outliers,\"Voltage AN (volts)\",Fill_nan = False)\n",
    "        df.loc[idk_outlier, \"Voltage BN (volts)\"] = weighted_knn(df[columns], idk_outlier, list_outliers, \"Voltage BN (volts)\",Fill_nan = False)\n",
    "        df.loc[idk_outlier, \"Voltage CN (volts)\"] = weighted_knn(df[columns], idk_outlier, list_outliers, \"Voltage CN (volts)\",Fill_nan = False)\n",
    "\n",
    "        # Visualize the outcome\n",
    "        print(\"\\n\", 20 * \"-\", \"New data (with the filled value(s))\",  20 * \"-\")\n",
    "        display(string_inv_data[string_inv_names[idk_special]].loc[idk_outlier][columns])\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save outcomes as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FINAL TASK: Save the cleared datasets \n",
    "saving_folder_name = \"Cleaned\"\n",
    "saving_string_inv_name = \"String Inverters\"\n",
    "saving_folder_path = path.join(system_path, saving_folder_name)\n",
    "saving_stringInv_folder_path = path.join(system_path, saving_folder_name, saving_string_inv_name)\n",
    "\n",
    "print(f\"TIME: {datetime.now().strftime('%H:%M')}\",f\"\\nPV System --> {system_name.upper()}\")\n",
    "\n",
    "# Create the main saving folder\n",
    "if not path.exists(saving_folder_path):\n",
    "    makedirs(saving_folder_path)\n",
    "    print(f\"A new saving folder has been created: {saving_folder_path}\\n\")\n",
    "\n",
    "# Create the saving folder for the cleaned inverter data\n",
    "if not path.exists(saving_stringInv_folder_path):\n",
    "    makedirs(saving_stringInv_folder_path)\n",
    "    print(f\"A new saving folder has been created: {saving_stringInv_folder_path}\\n\")\n",
    "    \n",
    "# Save the files as CSV files \n",
    "for inv_name in inv_names:\n",
    "    \n",
    "    # DF: System\n",
    "    file_name = f\"cleaned_{inv_name.upper()}_data.csv\"\n",
    "    inv_data[inv_name].to_csv(path.join(saving_folder_path, file_name), index=False)\n",
    "    print(f\"--> The cleaned data for '{inv_name}' has been saved.\\n\")\n",
    "    \n",
    "     # DFs: String inverter \n",
    "    for string_inv_name in string_inv_names:\n",
    "        file_name = f\"cleaned_INV_{string_inv_name.replace('INV', '')}_data.csv\"\n",
    "        string_inv_data[string_inv_name].to_csv(path.join(saving_stringInv_folder_path, file_name), index=False)\n",
    "        print(f\"--> The cleaned data for '{string_inv_name}' has been saved.\")\n",
    "        \n",
    "print(\"\\n\",\"-\"*40, \"FINISHED\", \"-\"*40)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sample]",
   "language": "python",
   "name": "conda-env-sample-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
